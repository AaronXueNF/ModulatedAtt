[1;31m
________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/

[0;33m
WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...
[m
]0;root@678f520f5371: /home/MultiRate_AttModulationroot@678f520f5371:/home/MultiRate_AttModulation# python train.py --json_args ./hyperparam/train_AdaScaleMAtt_mse.json 
[Preparing] Using Net: cheng2020_AdaptScaleMAtt_woGMM
[Preparing] start new training.
[Training] Start from epoch 0
[training] In epoch 0, lr: 0.0001, Aux lr: 0.001
[Training] epoch   0: [   0/14786] | RD Loss: 145.6004 | R:10.2072, D:75218.4297 | Aux loss: 13196.13 | Batch time: 1.4873 |
[Training] epoch   0: [ 500/14786] | RD Loss: 50.9812 | R:1.1321, D:1314.0800 | Aux loss: 12918.20 | Batch time: 0.6040 |
[Training] epoch   0: [1000/14786] | RD Loss: 36.2670 | R:1.1110, D:875.7464 | Aux loss: 12615.38 | Batch time: 0.6091 |
[Training] epoch   0: [1500/14786] | RD Loss: 30.3127 | R:1.0903, D:702.5489 | Aux loss: 12283.83 | Batch time: 0.6152 |
[Training] epoch   0: [2000/14786] | RD Loss: 26.8389 | R:1.0686, D:607.5257 | Aux loss: 11948.23 | Batch time: 0.6278 |
[Training] epoch   0: [2500/14786] | RD Loss: 24.3538 | R:1.0476, D:544.1196 | Aux loss: 11562.88 | Batch time: 0.6260 |
[Training] epoch   0: [3000/14786] | RD Loss: 22.6768 | R:1.0297, D:499.6930 | Aux loss: 11091.10 | Batch time: 0.6268 |
[Training] epoch   0: [3500/14786] | RD Loss: 21.4847 | R:1.0135, D:466.9120 | Aux loss: 10577.50 | Batch time: 0.6268 |
[Training] epoch   0: [4000/14786] | RD Loss: 20.8152 | R:1.0018, D:446.2484 | Aux loss: 10066.59 | Batch time: 0.6295 |
[Training] epoch   0: [4500/14786] | RD Loss: 20.8485 | R:0.9880, D:440.0778 | Aux loss: 9629.41 | Batch time: 0.6316 |
[Training] epoch   0: [5000/14786] | RD Loss: 20.4539 | R:0.9737, D:426.3198 | Aux loss: 8979.31 | Batch time: 0.6335 |
[Training] epoch   0: [5500/14786] | RD Loss: 19.8151 | R:0.9612, D:410.5767 | Aux loss: 8113.82 | Batch time: 0.6348 |
[Training] epoch   0: [6000/14786] | RD Loss: 19.1776 | R:0.9493, D:396.4602 | Aux loss: 7243.96 | Batch time: 0.6316 |
[Training] epoch   0: [6500/14786] | RD Loss: 18.6845 | R:0.9405, D:386.2020 | Aux loss: 6424.41 | Batch time: 0.6292 |
[Training] epoch   0: [7000/14786] | RD Loss: 18.2599 | R:0.9311, D:376.2191 | Aux loss: 5380.61 | Batch time: 0.6299 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   0: RD Loss: 14.5698 | best RD loss: 14.5698 | R loss: 0.4241 | D Loss: 250.8668 | Aux loss: 4383.1055
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740     0.00960     0.01180  ...     0.04830     0.05770     0.06710     0.08015     0.09320     0.11135     0.12950     0.15475     0.18000
rd_loss    0.706306    0.832609    0.959874    1.134128    1.303042    1.468545    1.613454    1.867371    2.131671  ...   17.006859   21.789319   27.226029   31.873916   36.474294   38.377647   38.934039   39.851937   39.633737
r_loss     0.321860    0.325554    0.329356    0.345818    0.363137    0.381505    0.400791    0.419422    0.439205  ...    0.475344    0.469726    0.464162    0.458654    0.453449    0.446508    0.439855    0.435479    0.431352
d_loss   213.581022  215.767976  217.419999  210.216105  204.327082  181.173320  163.873317  150.828106  143.429331  ...  342.267375  369.490338  398.835543  391.955857  386.489768  340.647856  297.252381  254.710546  217.791017

[3 rows x 23 columns]
[Training] epoch   0: [7500/14786] | RD Loss: 17.8592 | R:0.9224, D:366.6315 | Aux loss: 4188.55 | Batch time: 0.6301 |
[Training] epoch   0: [8000/14786] | RD Loss: 17.5669 | R:0.9150, D:359.2570 | Aux loss: 3169.17 | Batch time: 0.6289 |
[Training] epoch   0: [8500/14786] | RD Loss: 17.2323 | R:0.9086, D:352.7917 | Aux loss: 2128.43 | Batch time: 0.6288 |
[Training] epoch   0: [9000/14786] | RD Loss: 16.8588 | R:0.9034, D:345.9339 | Aux loss: 654.82 | Batch time: 0.6295 |
[Training] epoch   0: [9500/14786] | RD Loss: 16.5821 | R:0.8986, D:340.7945 | Aux loss: 326.11 | Batch time: 0.6284 |
[Training] epoch   0: [10000/14786] | RD Loss: 16.3422 | R:0.8937, D:335.2087 | Aux loss: 1272.99 | Batch time: 0.6295 |
[Training] epoch   0: [10500/14786] | RD Loss: 16.2129 | R:0.8888, D:330.9754 | Aux loss: 1999.33 | Batch time: 0.6299 |
[Training] epoch   0: [11000/14786] | RD Loss: 15.9431 | R:0.8843, D:325.4083 | Aux loss: 2708.36 | Batch time: 0.6295 |
[Training] epoch   0: [11500/14786] | RD Loss: 15.6881 | R:0.8803, D:320.0817 | Aux loss: 3762.16 | Batch time: 0.6295 |
[Training] epoch   0: [12000/14786] | RD Loss: 15.4125 | R:0.8783, D:314.8282 | Aux loss: 4693.82 | Batch time: 0.6291 |
[Training] epoch   0: [12500/14786] | RD Loss: 15.2121 | R:0.8762, D:310.9239 | Aux loss: 5741.08 | Batch time: 0.6296 |
[Training] epoch   0: [13000/14786] | RD Loss: 14.9891 | R:0.8743, D:306.6205 | Aux loss: 6890.31 | Batch time: 0.6301 |
[Training] epoch   0: [13500/14786] | RD Loss: 14.7907 | R:0.8720, D:302.5018 | Aux loss: 7999.79 | Batch time: 0.6306 |
[Training] epoch   0: [14000/14786] | RD Loss: 14.6097 | R:0.8704, D:298.8892 | Aux loss: 9112.29 | Batch time: 0.6309 |
[Training] epoch   0: [14500/14786] | RD Loss: 14.4621 | R:0.8689, D:295.4939 | Aux loss: 10252.06 | Batch time: 0.6313 |
[Evaluating] epoch   0: RD Loss: 15.1902 | best RD loss: 14.5698 | R loss: 0.4681 | D Loss: 323.6655 | Aux loss: 10786.4434
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740     0.00960     0.01180  ...     0.04830     0.05770     0.06710     0.08015     0.09320     0.11135     0.12950     0.15475     0.18000
rd_loss    0.714320    0.871700    1.038729    1.305558    1.588600    1.818812    1.996827    2.233508    2.630924  ...   32.922502   33.400229   33.261942   28.216996   24.638286   23.793163   26.732153   31.342549   36.017158
r_loss     0.275918    0.285350    0.295719    0.319360    0.344836    0.369218    0.394287    0.418150    0.445697  ...    0.564594    0.573721    0.582699    0.580924    0.579330    0.566111    0.552925    0.536511    0.521414
d_loss   243.556667  249.510913  256.210437  262.986123  270.383415  241.598948  216.559355  189.099801  185.188721  ...  669.935966  568.916951  487.022971  344.804398  258.143309  208.594993  202.156208  199.069707  197.198573

[3 rows x 23 columns]
[training] In epoch 1, lr: 0.0001, Aux lr: 0.001
[Training] epoch   1: [   0/14786] | RD Loss: 51.3754 | R:1.1191, D:388.0799 | Aux loss: 10788.46 | Batch time: 0.7273 |
[Training] epoch   1: [ 500/14786] | RD Loss: 14.1290 | R:0.8204, D:268.3641 | Aux loss: 11690.02 | Batch time: 0.6281 |
[Training] epoch   1: [1000/14786] | RD Loss: 12.4305 | R:0.8046, D:246.1340 | Aux loss: 12494.76 | Batch time: 0.6246 |
[Training] epoch   1: [1500/14786] | RD Loss: 11.7765 | R:0.8080, D:233.3152 | Aux loss: 13516.43 | Batch time: 0.6309 |
[Training] epoch   1: [2000/14786] | RD Loss: 11.1760 | R:0.8087, D:219.8952 | Aux loss: 14966.75 | Batch time: 0.6341 |
[Training] epoch   1: [2500/14786] | RD Loss: 10.4856 | R:0.8135, D:206.6257 | Aux loss: 16974.71 | Batch time: 0.6364 |
[Training] epoch   1: [3000/14786] | RD Loss: 9.9447 | R:0.8172, D:195.8305 | Aux loss: 18395.22 | Batch time: 0.6371 |
[Training] epoch   1: [3500/14786] | RD Loss: 9.5195 | R:0.8215, D:187.3506 | Aux loss: 19931.11 | Batch time: 0.6381 |
[Training] epoch   1: [4000/14786] | RD Loss: 9.2873 | R:0.8228, D:182.3377 | Aux loss: 21198.14 | Batch time: 0.6389 |
[Training] epoch   1: [4500/14786] | RD Loss: 9.1744 | R:0.8248, D:179.6146 | Aux loss: 22129.24 | Batch time: 0.6396 |
[Training] epoch   1: [5000/14786] | RD Loss: 9.1300 | R:0.8287, D:178.1441 | Aux loss: 22881.92 | Batch time: 0.6400 |
[Training] epoch   1: [5500/14786] | RD Loss: 8.9928 | R:0.8314, D:175.3587 | Aux loss: 23869.70 | Batch time: 0.6403 |
[Training] epoch   1: [6000/14786] | RD Loss: 8.8084 | R:0.8332, D:171.7819 | Aux loss: 24814.78 | Batch time: 0.6410 |
[Training] epoch   1: [6500/14786] | RD Loss: 8.7276 | R:0.8366, D:169.8624 | Aux loss: 25834.80 | Batch time: 0.6413 |
[Training] epoch   1: [7000/14786] | RD Loss: 8.6070 | R:0.8406, D:167.5975 | Aux loss: 26340.98 | Batch time: 0.6417 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   1: RD Loss: 8.7263 | best RD loss: 8.7263 | R loss: 0.4876 | D Loss: 124.9549 | Aux loss: 26654.8027
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830     0.05770     0.06710     0.08015     0.09320     0.11135     0.12950     0.15475     0.18000
rd_loss    0.538231    0.600183    0.640774    0.693985   0.724606   0.789401   0.867725   1.015168   1.167736   1.379385  ...   3.558294   4.970865    6.927302   10.493208   15.182887   20.724994   26.471656   32.749941   32.860627   32.171410
r_loss     0.208861    0.224865    0.241827    0.270206   0.301790   0.333870   0.369551   0.408603   0.451553   0.489477  ...   0.602433   0.617239    0.624177    0.632019    0.635372    0.639827    0.636180    0.633725    0.615983    0.599040
d_loss   182.983630  159.709892  137.568005  113.007741  91.916380  75.921947  67.320725  63.183857  60.693418  57.974451  ...  75.308553  90.137176  109.239597  146.962571  181.503609  215.506092  232.020436  248.001663  208.366030  175.402046

[3 rows x 23 columns]
[Training] epoch   1: [7500/14786] | RD Loss: 8.5515 | R:0.8435, D:166.2437 | Aux loss: 26616.57 | Batch time: 0.6421 |
[Training] epoch   1: [8000/14786] | RD Loss: 8.4431 | R:0.8460, D:164.4256 | Aux loss: 26561.56 | Batch time: 0.6422 |
[Training] epoch   1: [8500/14786] | RD Loss: 8.3839 | R:0.8487, D:163.3360 | Aux loss: 25956.04 | Batch time: 0.6422 |
[Training] epoch   1: [9000/14786] | RD Loss: 8.2979 | R:0.8501, D:161.9132 | Aux loss: 25019.26 | Batch time: 0.6425 |
[Training] epoch   1: [9500/14786] | RD Loss: 8.1994 | R:0.8525, D:160.0524 | Aux loss: 23985.60 | Batch time: 0.6427 |
[Training] epoch   1: [10000/14786] | RD Loss: 8.1198 | R:0.8541, D:158.8968 | Aux loss: 22269.47 | Batch time: 0.6429 |
[Training] epoch   1: [10500/14786] | RD Loss: 8.0258 | R:0.8554, D:157.2423 | Aux loss: 20284.94 | Batch time: 0.6430 |
[Training] epoch   1: [11000/14786] | RD Loss: 7.9418 | R:0.8572, D:155.8496 | Aux loss: 16891.74 | Batch time: 0.6430 |
[Training] epoch   1: [11500/14786] | RD Loss: 7.8776 | R:0.8586, D:154.8891 | Aux loss: 11138.77 | Batch time: 0.6430 |
[Training] epoch   1: [12000/14786] | RD Loss: 7.7972 | R:0.8602, D:153.7039 | Aux loss: 447.99 | Batch time: 0.6416 |
[Training] epoch   1: [12500/14786] | RD Loss: 7.7281 | R:0.8623, D:152.6438 | Aux loss: 23.09 | Batch time: 0.6399 |
[Training] epoch   1: [13000/14786] | RD Loss: 7.6825 | R:0.8644, D:151.8055 | Aux loss: 24.36 | Batch time: 0.6386 |
[Training] epoch   1: [13500/14786] | RD Loss: 7.6079 | R:0.8665, D:150.5368 | Aux loss: 28.17 | Batch time: 0.6365 |
[Training] epoch   1: [14000/14786] | RD Loss: 7.5692 | R:0.8680, D:149.7967 | Aux loss: 28.00 | Batch time: 0.6355 |
[Training] epoch   1: [14500/14786] | RD Loss: 7.5809 | R:0.8694, D:150.0320 | Aux loss: 28.04 | Batch time: 0.6356 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   1: RD Loss: 3.3231 | best RD loss: 3.3231 | R loss: 0.5363 | D Loss: 84.6199 | Aux loss: 30.4601
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740     0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.459684    0.555087    0.659413    0.800288    0.945128    1.077455    1.219793    1.386785   1.533009   1.768075  ...   2.950231   3.173163   3.267975   3.728573   4.838634   6.859640   7.986127   9.079735   8.380878   8.887233
r_loss     0.218035    0.238738    0.260768    0.286671    0.314906    0.345867    0.379500    0.415259   0.453023   0.486949  ...   0.623901   0.659885   0.692139   0.731189   0.756935   0.793270   0.789411   0.786132   0.733659   0.697552
d_loss   134.249216  134.616410  137.463836  136.964665  137.004804  121.931376  113.553056  101.200564  91.524226  83.460994  ...  59.269565  52.034750  44.641876  44.670395  50.925744  65.089812  64.631488  64.043266  49.416600  45.498226

[3 rows x 23 columns]
[training] In epoch 2, lr: 0.0001, Aux lr: 0.001
[Training] epoch   2: [   0/14786] | RD Loss: 8.2325 | R:1.1627, D:88.2067 | Aux loss: 30.27 | Batch time: 0.7352 |
[Training] epoch   2: [ 500/14786] | RD Loss: 5.8040 | R:0.9063, D:124.1901 | Aux loss: 35.95 | Batch time: 0.6507 |
[Training] epoch   2: [1000/14786] | RD Loss: 5.7869 | R:0.9129, D:121.7854 | Aux loss: 42.14 | Batch time: 0.6343 |
[Training] epoch   2: [1500/14786] | RD Loss: 5.8604 | R:0.9202, D:122.8854 | Aux loss: 29.25 | Batch time: 0.6205 |
[Training] epoch   2: [2000/14786] | RD Loss: 5.7998 | R:0.9234, D:124.0368 | Aux loss: 30.49 | Batch time: 0.6186 |
[Training] epoch   2: [2500/14786] | RD Loss: 5.7695 | R:0.9276, D:123.0209 | Aux loss: 33.64 | Batch time: 0.6203 |
[Training] epoch   2: [3000/14786] | RD Loss: 5.6871 | R:0.9274, D:120.6310 | Aux loss: 35.32 | Batch time: 0.6226 |
[Training] epoch   2: [3500/14786] | RD Loss: 5.6362 | R:0.9284, D:118.6877 | Aux loss: 31.51 | Batch time: 0.6245 |
[Training] epoch   2: [4000/14786] | RD Loss: 5.5987 | R:0.9306, D:118.1142 | Aux loss: 33.87 | Batch time: 0.6264 |
[Training] epoch   2: [4500/14786] | RD Loss: 5.5755 | R:0.9311, D:117.6385 | Aux loss: 40.17 | Batch time: 0.6238 |
[Training] epoch   2: [5000/14786] | RD Loss: 5.5171 | R:0.9324, D:116.2303 | Aux loss: 46.30 | Batch time: 0.6221 |
[Training] epoch   2: [5500/14786] | RD Loss: 5.5253 | R:0.9335, D:116.4492 | Aux loss: 33.95 | Batch time: 0.6196 |
[Training] epoch   2: [6000/14786] | RD Loss: 5.5510 | R:0.9350, D:116.7169 | Aux loss: 40.67 | Batch time: 0.6175 |
[Training] epoch   2: [6500/14786] | RD Loss: 5.5417 | R:0.9360, D:116.3150 | Aux loss: 37.29 | Batch time: 0.6145 |
[Training] epoch   2: [7000/14786] | RD Loss: 5.5268 | R:0.9371, D:115.9118 | Aux loss: 38.69 | Batch time: 0.6123 |
[Evaluating] epoch   2: RD Loss: 5.1005 | best RD loss: 3.3231 | R loss: 0.5384 | D Loss: 87.8979 | Aux loss: 42.3644
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925     0.04830     0.05770     0.06710     0.08015     0.09320     0.11135    0.12950    0.15475    0.18000
rd_loss    0.379655   0.439801   0.496346   0.581382   0.672030   0.795919   0.916599   1.056930   1.184558   1.407662  ...   4.481589    5.688583    6.876786    8.023922    9.783131   11.479343   12.996040  13.095127  14.458475  15.513356
r_loss     0.194799   0.215553   0.238628   0.267696   0.300929   0.335106   0.372831   0.409673   0.449197   0.484260  ...   0.632471    0.667829    0.693433    0.720674    0.745237    0.773221    0.795762   0.824639   0.801523   0.778503
d_loss   102.697478  95.424501  88.868286  83.649797  80.674107  76.802068  73.482161  67.422587  62.318674  60.156482  ...  98.066708  103.949345  107.163824  108.841234  112.762245  114.872554  109.566929  94.752799  88.251706  81.860288

[3 rows x 23 columns]
[Training] epoch   2: [7500/14786] | RD Loss: 5.5015 | R:0.9368, D:115.5566 | Aux loss: 33.48 | Batch time: 0.6104 |
[Training] epoch   2: [8000/14786] | RD Loss: 5.4934 | R:0.9373, D:115.3850 | Aux loss: 40.06 | Batch time: 0.6084 |
[Training] epoch   2: [8500/14786] | RD Loss: 5.4753 | R:0.9379, D:115.0654 | Aux loss: 46.63 | Batch time: 0.6067 |
[Training] epoch   2: [9000/14786] | RD Loss: 5.4592 | R:0.9384, D:114.5714 | Aux loss: 39.87 | Batch time: 0.6052 |
[Training] epoch   2: [9500/14786] | RD Loss: 5.4392 | R:0.9392, D:114.0812 | Aux loss: 37.82 | Batch time: 0.6036 |
[Training] epoch   2: [10000/14786] | RD Loss: 5.4232 | R:0.9403, D:113.7752 | Aux loss: 38.44 | Batch time: 0.6024 |
[Training] epoch   2: [10500/14786] | RD Loss: 5.4086 | R:0.9407, D:113.2313 | Aux loss: 53.18 | Batch time: 0.6013 |
[Training] epoch   2: [11000/14786] | RD Loss: 5.3998 | R:0.9410, D:112.8525 | Aux loss: 38.34 | Batch time: 0.6002 |
[Training] epoch   2: [11500/14786] | RD Loss: 5.3961 | R:0.9423, D:112.8338 | Aux loss: 45.14 | Batch time: 0.5992 |
[Training] epoch   2: [12000/14786] | RD Loss: 5.3741 | R:0.9432, D:112.4690 | Aux loss: 50.34 | Batch time: 0.5985 |
[Training] epoch   2: [12500/14786] | RD Loss: 5.3539 | R:0.9434, D:112.1337 | Aux loss: 61.55 | Batch time: 0.5976 |
[Training] epoch   2: [13000/14786] | RD Loss: 5.3532 | R:0.9441, D:111.8581 | Aux loss: 45.47 | Batch time: 0.5969 |
[Training] epoch   2: [13500/14786] | RD Loss: 5.3439 | R:0.9444, D:111.6360 | Aux loss: 52.39 | Batch time: 0.5962 |
[Training] epoch   2: [14000/14786] | RD Loss: 5.3425 | R:0.9448, D:111.5357 | Aux loss: 60.21 | Batch time: 0.5955 |
[Training] epoch   2: [14500/14786] | RD Loss: 5.3250 | R:0.9454, D:111.1435 | Aux loss: 60.95 | Batch time: 0.5949 |
[Evaluating] epoch   2: RD Loss: 19.4170 | best RD loss: 3.3231 | R loss: 0.6067 | D Loss: 220.2352 | Aux loss: 51.9007
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770     0.06710     0.08015     0.09320     0.11135     0.12950     0.15475     0.18000
rd_loss    0.761421    0.867765    0.958240    1.035428    1.099979    1.163137    1.199621   1.266352   1.279702   1.396110  ...   2.415408   2.791569   4.263696    8.604326   17.042638   34.040584   53.435893   81.855080  101.983521  123.453127
r_loss     0.210396    0.230525    0.253107    0.276335    0.303043    0.337948    0.377406   0.420312   0.465303   0.512629  ...   0.700652   0.748568   0.783496    0.823679    0.864104    0.912918    0.950102    0.990906    0.986564    0.980879
d_loss   306.125167  271.165642  243.149214  202.424781  173.246922  137.531356  111.110183  88.129118  69.016933  57.555769  ...  43.688049  42.298164  60.315430  115.955987  201.853192  355.447068  471.358698  624.433767  652.645910  680.401358

[3 rows x 23 columns]
[training] In epoch 3, lr: 0.0001, Aux lr: 0.001
[Training] epoch   3: [   0/14786] | RD Loss: 5.1754 | R:1.0203, D:86.0257 | Aux loss: 50.42 | Batch time: 0.7740 |
[Training] epoch   3: [ 500/14786] | RD Loss: 5.4651 | R:0.9606, D:110.3466 | Aux loss: 52.99 | Batch time: 0.5794 |
[Training] epoch   3: [1000/14786] | RD Loss: 5.3553 | R:0.9644, D:110.8067 | Aux loss: 63.41 | Batch time: 0.5782 |
[Training] epoch   3: [1500/14786] | RD Loss: 5.3095 | R:0.9641, D:109.4784 | Aux loss: 57.49 | Batch time: 0.5790 |
[Training] epoch   3: [2000/14786] | RD Loss: 5.3144 | R:0.9641, D:109.1865 | Aux loss: 63.73 | Batch time: 0.5792 |
[Training] epoch   3: [2500/14786] | RD Loss: 5.2950 | R:0.9650, D:108.3548 | Aux loss: 59.06 | Batch time: 0.5786 |
[Training] epoch   3: [3000/14786] | RD Loss: 5.2215 | R:0.9645, D:107.1934 | Aux loss: 52.05 | Batch time: 0.5789 |
[Training] epoch   3: [3500/14786] | RD Loss: 5.1346 | R:0.9646, D:105.8214 | Aux loss: 60.30 | Batch time: 0.5791 |
[Training] epoch   3: [4000/14786] | RD Loss: 5.0873 | R:0.9647, D:104.7506 | Aux loss: 86.13 | Batch time: 0.5787 |
[Training] epoch   3: [4500/14786] | RD Loss: 5.0757 | R:0.9658, D:104.3280 | Aux loss: 68.32 | Batch time: 0.5788 |
[Training] epoch   3: [5000/14786] | RD Loss: 95878.3412 | R:0.9676, D:1661685.9082 | Aux loss: 67.32 | Batch time: 0.5789 |
[Training] epoch   3: [5500/14786] | RD Loss: 87164.1182 | R:0.9665, D:1510659.4447 | Aux loss: 64.98 | Batch time: 0.5785 |
[Training] epoch   3: [6000/14786] | RD Loss: 79902.0095 | R:0.9655, D:1384799.3708 | Aux loss: 75.21 | Batch time: 0.5785 |
[Training] epoch   3: [6500/14786] | RD Loss: 73757.0150 | R:0.9656, D:1278300.1063 | Aux loss: 84.98 | Batch time: 0.5786 |
[Training] epoch   3: [7000/14786] | RD Loss: 68489.7243 | R:0.9663, D:1187012.4217 | Aux loss: 67.70 | Batch time: 0.5784 |
[Evaluating] epoch   3: RD Loss: 3.3420 | best RD loss: 3.3231 | R loss: 0.5828 | D Loss: 82.1024 | Aux loss: 69.1348
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600     0.00740     0.00960     0.01180     0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.360531   0.420350   0.479796   0.575609   0.686566   0.882441    1.118770    1.420458    1.770608    2.192356  ...   4.147106   4.801283   5.356935   5.921368   6.385460   6.716056   6.441245   5.784586   6.070148   6.429395
r_loss     0.177745   0.196829   0.218573   0.247718   0.281649   0.319529    0.362431    0.406293    0.453445    0.500754  ...   0.696187   0.745277   0.781118   0.830854   0.857074   0.885768   0.897663   0.925280   0.913390   0.902715
d_loss   101.548007  95.115385  90.076995  87.437610  88.025341  93.818805  102.208007  105.642135  111.624013  110.202096  ...  87.921509  83.975275  79.303584  75.864585  68.975497  62.556749  49.785208  37.523605  33.323154  30.703776

[3 rows x 23 columns]
[Training] epoch   3: [7500/14786] | RD Loss: 63924.6535 | R:0.9662, D:1107895.0251 | Aux loss: 70.90 | Batch time: 0.5785 |
[Training] epoch   3: [8000/14786] | RD Loss: 59930.1483 | R:0.9665, D:1038666.1370 | Aux loss: 74.92 | Batch time: 0.5786 |
[Training] epoch   3: [8500/14786] | RD Loss: 56405.5132 | R:0.9679, D:977580.4911 | Aux loss: 82.87 | Batch time: 0.5783 |
[Training] epoch   3: [9000/14786] | RD Loss: 53272.4688 | R:0.9685, D:923281.6513 | Aux loss: 81.96 | Batch time: 0.5784 |
[Training] epoch   3: [9500/14786] | RD Loss: 50469.1721 | R:0.9683, D:874697.7214 | Aux loss: 75.43 | Batch time: 0.5785 |
[Training] epoch   3: [10000/14786] | RD Loss: 47946.2016 | R:0.9687, D:830971.8387 | Aux loss: 87.29 | Batch time: 0.5783 |
[Training] epoch   3: [10500/14786] | RD Loss: 45663.4788 | R:0.9690, D:791410.0147 | Aux loss: 86.93 | Batch time: 0.5784 |
[Training] epoch   3: [11000/14786] | RD Loss: 43588.2774 | R:0.9695, D:755444.3491 | Aux loss: 76.75 | Batch time: 0.5784 |
[Training] epoch   3: [11500/14786] | RD Loss: 41693.4795 | R:0.9696, D:722605.5515 | Aux loss: 84.84 | Batch time: 0.5783 |
[Training] epoch   3: [12000/14786] | RD Loss: 39956.5701 | R:0.9694, D:692503.0416 | Aux loss: 83.22 | Batch time: 0.5784 |
[Training] epoch   3: [12500/14786] | RD Loss: 38358.5931 | R:0.9702, D:664808.4060 | Aux loss: 92.86 | Batch time: 0.5784 |
[Training] epoch   3: [13000/14786] | RD Loss: 36883.5386 | R:0.9706, D:639244.1612 | Aux loss: 84.37 | Batch time: 0.5784 |
[Training] epoch   3: [13500/14786] | RD Loss: 35517.7374 | R:0.9708, D:615573.4939 | Aux loss: 90.73 | Batch time: 0.5784 |
[Training] epoch   3: [14000/14786] | RD Loss: 34249.4911 | R:0.9712, D:593593.4875 | Aux loss: 101.22 | Batch time: 0.5784 |
[Training] epoch   3: [14500/14786] | RD Loss: 33068.7044 | R:0.9715, D:573129.2466 | Aux loss: 95.73 | Batch time: 0.5784 |
[Evaluating] epoch   3: RD Loss: 5.2737 | best RD loss: 3.3231 | R loss: 0.5976 | D Loss: 99.6039 | Aux loss: 95.9157
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830     0.05770     0.06710     0.08015     0.09320     0.11135    0.12950    0.15475    0.18000
rd_loss    0.468069    0.541245    0.610697    0.710386    0.805670    0.954492    1.096496   1.241345   1.331918   1.338988  ...   3.100474   4.894558    6.680304    9.598533   11.804786   14.401176   13.951174  13.730562  13.856403  15.156223
r_loss     0.171372    0.190625    0.212454    0.238776    0.269339    0.304645    0.344356   0.388402   0.435398   0.483666  ...   0.714117   0.780565    0.813570    0.855090    0.886050    0.923628    0.961789   1.008358   0.995841   0.986401
d_loss   164.831674  149.199728  137.325232  125.762591  116.593841  108.307835  101.640467  88.848176  75.976272  55.721272  ...  60.798886  85.175823  101.676493  130.304650  136.228764  144.608891  116.653661  98.240958  83.105405  78.721228

[3 rows x 23 columns]
[training] In epoch 4, lr: 0.0001, Aux lr: 0.001
[Training] epoch   4: [   0/14786] | RD Loss: 2.3651 | R:0.8490, D:61.7529 | Aux loss: 93.12 | Batch time: 0.7754 |
[Training] epoch   4: [ 500/14786] | RD Loss: 4.1435 | R:0.9761, D:85.1985 | Aux loss: 103.81 | Batch time: 0.5807 |
[Training] epoch   4: [1000/14786] | RD Loss: 4.0790 | R:0.9756, D:84.0959 | Aux loss: 87.21 | Batch time: 0.5796 |
[Training] epoch   4: [1500/14786] | RD Loss: 4.0624 | R:0.9787, D:84.4826 | Aux loss: 96.63 | Batch time: 0.5785 |
[Training] epoch   4: [2000/14786] | RD Loss: 4.1022 | R:0.9802, D:84.9697 | Aux loss: 97.01 | Batch time: 0.5789 |
[Training] epoch   4: [2500/14786] | RD Loss: 4.1109 | R:0.9809, D:85.3450 | Aux loss: 128.31 | Batch time: 0.5789 |
[Training] epoch   4: [3000/14786] | RD Loss: 4.1137 | R:0.9837, D:85.4618 | Aux loss: 98.81 | Batch time: 0.5783 |
[Training] epoch   4: [3500/14786] | RD Loss: 4.1051 | R:0.9852, D:85.2440 | Aux loss: 94.21 | Batch time: 0.5786 |
[Training] epoch   4: [4000/14786] | RD Loss: 4.0801 | R:0.9869, D:84.9042 | Aux loss: 105.28 | Batch time: 0.5787 |
[Training] epoch   4: [4500/14786] | RD Loss: 4.0623 | R:0.9889, D:84.4782 | Aux loss: 115.23 | Batch time: 0.5787 |
[Training] epoch   4: [5000/14786] | RD Loss: 4.0597 | R:0.9914, D:84.3372 | Aux loss: 94.02 | Batch time: 0.5789 |
[Training] epoch   4: [5500/14786] | RD Loss: 4.0635 | R:0.9927, D:84.4646 | Aux loss: 112.64 | Batch time: 0.5790 |
[Training] epoch   4: [6000/14786] | RD Loss: 4.0659 | R:0.9939, D:84.4326 | Aux loss: 108.49 | Batch time: 0.5786 |
[Training] epoch   4: [6500/14786] | RD Loss: 4.0669 | R:0.9953, D:84.4324 | Aux loss: 98.72 | Batch time: 0.5787 |
[Training] epoch   4: [7000/14786] | RD Loss: 4.0482 | R:0.9959, D:84.1781 | Aux loss: 101.02 | Batch time: 0.5789 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   4: RD Loss: 2.6403 | best RD loss: 2.6403 | R loss: 0.5846 | D Loss: 66.3893 | Aux loss: 102.5648
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.384600    0.442096    0.493291   0.573712   0.651378   0.767721   0.876065   1.035207   1.189978   1.403393  ...   2.865308   3.529018   3.839108   4.190790   4.746799   5.236482   5.212982   4.976749   5.726546   6.664694
r_loss     0.146716    0.162353    0.180158   0.202823   0.229225   0.260078   0.294285   0.335116   0.381124   0.437692  ...   0.706472   0.782960   0.836837   0.896081   0.927945   0.967853   0.992592   1.025528   1.000556   0.975223
d_loss   132.158090  119.039627  107.976926  98.903614  91.772399  84.607097  78.618944  72.926220  68.546909  62.912067  ...  55.002192  56.854194  52.032419  49.101464  47.646338  45.800744  37.902023  30.511358  30.539515  31.608168

[3 rows x 23 columns]
[Training] epoch   4: [7500/14786] | RD Loss: 4.0267 | R:0.9966, D:83.8439 | Aux loss: 104.85 | Batch time: 0.5787 |
[Training] epoch   4: [8000/14786] | RD Loss: 4.0176 | R:0.9982, D:83.6725 | Aux loss: 122.27 | Batch time: 0.5788 |
[Training] epoch   4: [8500/14786] | RD Loss: 3.9993 | R:0.9998, D:83.2983 | Aux loss: 109.79 | Batch time: 0.5788 |
[Training] epoch   4: [9000/14786] | RD Loss: 3.9861 | R:1.0008, D:83.0354 | Aux loss: 110.85 | Batch time: 0.5786 |
[Training] epoch   4: [9500/14786] | RD Loss: 3.9703 | R:1.0018, D:82.7181 | Aux loss: 114.29 | Batch time: 0.5787 |
[Training] epoch   4: [10000/14786] | RD Loss: 3.9585 | R:1.0025, D:82.4850 | Aux loss: 129.56 | Batch time: 0.5788 |
[Training] epoch   4: [10500/14786] | RD Loss: 3.9499 | R:1.0045, D:82.2740 | Aux loss: 107.30 | Batch time: 0.5786 |
[Training] epoch   4: [11000/14786] | RD Loss: 3.9375 | R:1.0055, D:82.1750 | Aux loss: 118.50 | Batch time: 0.5787 |
[Training] epoch   4: [11500/14786] | RD Loss: 3.9239 | R:1.0063, D:81.9261 | Aux loss: 118.95 | Batch time: 0.5788 |
[Training] epoch   4: [12000/14786] | RD Loss: 3.9098 | R:1.0073, D:81.6774 | Aux loss: 119.05 | Batch time: 0.5787 |
[Training] epoch   4: [12500/14786] | RD Loss: 3.8988 | R:1.0083, D:81.4169 | Aux loss: 113.10 | Batch time: 0.5787 |
[Training] epoch   4: [13000/14786] | RD Loss: 3.8895 | R:1.0097, D:81.3813 | Aux loss: 115.91 | Batch time: 0.5788 |
[Training] epoch   4: [13500/14786] | RD Loss: 3.8745 | R:1.0104, D:81.1321 | Aux loss: 126.62 | Batch time: 0.5787 |
[Training] epoch   4: [14000/14786] | RD Loss: 3.8680 | R:1.0112, D:81.0740 | Aux loss: 124.93 | Batch time: 0.5787 |
[Training] epoch   4: [14500/14786] | RD Loss: 3.8587 | R:1.0127, D:80.8850 | Aux loss: 133.39 | Batch time: 0.5787 |
[Evaluating] epoch   4: RD Loss: 6.0005 | best RD loss: 2.6403 | R loss: 0.6821 | D Loss: 87.6788 | Aux loss: 122.8269
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770     0.06710     0.08015     0.09320     0.11135     0.12950     0.15475     0.18000
rd_loss    0.394242    0.449126    0.496292   0.540680   0.586181   0.672446   0.751395   0.879499   1.010783   1.213303  ...   3.275014   4.571614   6.297418    8.573000   10.691217   13.179020   15.354054   18.136972   20.853601   24.465195
r_loss     0.163006    0.182553    0.204823   0.231282   0.261583   0.298951   0.340998   0.388922   0.442610   0.501218  ...   0.781063   0.866315   0.938034    1.018108    1.077081    1.145455    1.187388    1.242828    1.248232    1.264083
d_loss   128.464745  113.435014  100.506632  82.506103  70.564944  62.249032  55.459024  51.101759  48.150275  46.389857  ...  63.540155  76.714263  92.883605  112.591534  119.951798  129.115507  127.226457  130.456707  126.690590  128.895060

[3 rows x 23 columns]
[training] In epoch 5, lr: 0.0001, Aux lr: 0.001
[Training] epoch   5: [   0/14786] | RD Loss: 1.8430 | R:0.9045, D:79.5295 | Aux loss: 116.52 | Batch time: 0.7142 |
[Training] epoch   5: [ 500/14786] | RD Loss: 3.5617 | R:1.0600, D:74.0638 | Aux loss: 141.44 | Batch time: 0.5785 |
[Training] epoch   5: [1000/14786] | RD Loss: 3.5858 | R:1.0566, D:75.5310 | Aux loss: 118.56 | Batch time: 0.5795 |
[Training] epoch   5: [1500/14786] | RD Loss: 3.5043 | R:1.0581, D:74.2584 | Aux loss: 131.10 | Batch time: 0.5787 |
[Training] epoch   5: [2000/14786] | RD Loss: 3.4820 | R:1.0588, D:73.6484 | Aux loss: 137.32 | Batch time: 0.5783 |
[Training] epoch   5: [2500/14786] | RD Loss: 3.4987 | R:1.0594, D:74.1039 | Aux loss: 119.82 | Batch time: 0.5787 |
[Training] epoch   5: [3000/14786] | RD Loss: 3.4874 | R:1.0599, D:74.3359 | Aux loss: 122.81 | Batch time: 0.5786 |
[Training] epoch   5: [3500/14786] | RD Loss: 3.4902 | R:1.0605, D:74.4823 | Aux loss: 146.42 | Batch time: 0.5784 |
[Training] epoch   5: [4000/14786] | RD Loss: 3.4779 | R:1.0600, D:74.2667 | Aux loss: 126.64 | Batch time: 0.5786 |
[Training] epoch   5: [4500/14786] | RD Loss: 3.4826 | R:1.0620, D:74.4622 | Aux loss: 119.28 | Batch time: 0.5786 |
[Training] epoch   5: [5000/14786] | RD Loss: 3.4685 | R:1.0623, D:74.3013 | Aux loss: 130.33 | Batch time: 0.5784 |
[Training] epoch   5: [5500/14786] | RD Loss: 3.4552 | R:1.0623, D:74.0353 | Aux loss: 132.97 | Batch time: 0.5786 |
[Training] epoch   5: [6000/14786] | RD Loss: 3.4693 | R:1.0647, D:74.1406 | Aux loss: 136.16 | Batch time: 0.5786 |
[Training] epoch   5: [6500/14786] | RD Loss: 3.4621 | R:1.0672, D:73.7537 | Aux loss: 130.30 | Batch time: 0.5785 |
[Training] epoch   5: [7000/14786] | RD Loss: 3.4595 | R:1.0677, D:73.8374 | Aux loss: 127.32 | Batch time: 0.5786 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   5: RD Loss: 2.0511 | best RD loss: 2.0511 | R loss: 0.6369 | D Loss: 57.3423 | Aux loss: 137.4193
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.390204    0.459224    0.522768    0.628730    0.732933   0.862804   0.976380   1.095129   1.169925   1.277212  ...   1.897841   2.104521   2.289351   2.457968   2.701777   2.953669   3.451259   4.244856   5.403621   7.022957
r_loss     0.155881    0.175127    0.196636    0.221945    0.250628   0.285057   0.324213   0.367448   0.415140   0.468582  ...   0.727535   0.798360   0.858883   0.925082   0.989212   1.059841   1.111346   1.170268   1.177320   1.188252
d_loss   130.179488  120.892510  112.459301  108.476032  104.848817  96.291217  88.130617  75.800160  63.964812  52.679511  ...  29.816714  27.042671  24.791466  22.844804  21.367002  20.320054  21.014033  23.741996  27.310504  32.415027

[3 rows x 23 columns]
[Training] epoch   5: [7500/14786] | RD Loss: 3.4615 | R:1.0689, D:74.0090 | Aux loss: 131.02 | Batch time: 0.5786 |
[Training] epoch   5: [8000/14786] | RD Loss: 3.4499 | R:1.0689, D:73.7642 | Aux loss: 155.31 | Batch time: 0.5785 |
[Training] epoch   5: [8500/14786] | RD Loss: 3.4423 | R:1.0690, D:73.6252 | Aux loss: 128.51 | Batch time: 0.5786 |
[Training] epoch   5: [9000/14786] | RD Loss: 3.4366 | R:1.0698, D:73.6038 | Aux loss: 129.50 | Batch time: 0.5786 |
[Training] epoch   5: [9500/14786] | RD Loss: 3.4409 | R:1.0707, D:73.6900 | Aux loss: 139.77 | Batch time: 0.5784 |
[Training] epoch   5: [10000/14786] | RD Loss: 3.4482 | R:1.0715, D:73.8442 | Aux loss: 143.28 | Batch time: 0.5785 |
[Training] epoch   5: [10500/14786] | RD Loss: 3.4451 | R:1.0720, D:73.7587 | Aux loss: 143.91 | Batch time: 0.5786 |
[Training] epoch   5: [11000/14786] | RD Loss: 3.4466 | R:1.0726, D:73.8251 | Aux loss: 161.39 | Batch time: 0.5784 |
[Training] epoch   5: [11500/14786] | RD Loss: 3.4415 | R:1.0725, D:73.7273 | Aux loss: 158.65 | Batch time: 0.5785 |
[Training] epoch   5: [12000/14786] | RD Loss: 3.4399 | R:1.0733, D:73.7365 | Aux loss: 150.01 | Batch time: 0.5785 |
[Training] epoch   5: [12500/14786] | RD Loss: 3.4406 | R:1.0741, D:73.7283 | Aux loss: 136.16 | Batch time: 0.5784 |
[Training] epoch   5: [13000/14786] | RD Loss: 3.4436 | R:1.0747, D:73.8085 | Aux loss: 143.91 | Batch time: 0.5785 |
[Training] epoch   5: [13500/14786] | RD Loss: 3.4429 | R:1.0759, D:73.8312 | Aux loss: 148.26 | Batch time: 0.5785 |
[Training] epoch   5: [14000/14786] | RD Loss: 3.4393 | R:1.0770, D:73.7967 | Aux loss: 134.31 | Batch time: 0.5784 |
[Training] epoch   5: [14500/14786] | RD Loss: 3.4347 | R:1.0779, D:73.7432 | Aux loss: 142.46 | Batch time: 0.5784 |
[Evaluating] epoch   5: RD Loss: 2.5662 | best RD loss: 2.0511 | R loss: 0.6365 | D Loss: 54.8485 | Aux loss: 134.6711
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.367448    0.424879   0.474056   0.537387   0.587323   0.682074   0.773515   0.902113   1.035990   1.249859  ...   2.278939   2.601202   2.920080   3.234277   3.685941   4.136272   5.040834   6.277081   7.521083   9.127017
r_loss     0.157191    0.177298   0.200003   0.226590   0.257176   0.294068   0.335862   0.382797   0.435123   0.490466  ...   0.741126   0.805902   0.854712   0.907243   0.961561   1.018551   1.074687   1.135315   1.155232   1.180990
d_loss   116.809674  105.353494  94.500975  82.879214  71.771005  64.667670  59.142342  54.095403  50.920977  49.471875  ...  39.179942  37.169770  35.794943  34.680086  33.991013  33.451945  35.618746  39.704757  41.136353  44.144590

[3 rows x 23 columns]
[training] In epoch 6, lr: 0.0001, Aux lr: 0.001
[Training] epoch   6: [   0/14786] | RD Loss: 1.1094 | R:0.6289, D:80.0840 | Aux loss: 141.38 | Batch time: 0.7792 |
[Training] epoch   6: [ 500/14786] | RD Loss: 3.2272 | R:1.1087, D:69.8626 | Aux loss: 135.34 | Batch time: 0.5770 |
[Training] epoch   6: [1000/14786] | RD Loss: 3.2847 | R:1.0997, D:71.1242 | Aux loss: 150.54 | Batch time: 0.5775 |
[Training] epoch   6: [1500/14786] | RD Loss: 3.2985 | R:1.0963, D:71.4189 | Aux loss: 159.16 | Batch time: 0.5784 |
[Training] epoch   6: [2000/14786] | RD Loss: 3.3247 | R:1.0996, D:72.1490 | Aux loss: 180.21 | Batch time: 0.5778 |
[Training] epoch   6: [2500/14786] | RD Loss: 3.3314 | R:1.1024, D:72.1399 | Aux loss: 144.03 | Batch time: 0.5777 |
[Training] epoch   6: [3000/14786] | RD Loss: 3.3071 | R:1.1005, D:71.9340 | Aux loss: 142.96 | Batch time: 0.5782 |
[Training] epoch   6: [3500/14786] | RD Loss: 3.3120 | R:1.0993, D:72.0477 | Aux loss: 146.66 | Batch time: 0.5779 |
[Training] epoch   6: [4000/14786] | RD Loss: 3.2930 | R:1.1007, D:71.4614 | Aux loss: 167.21 | Batch time: 0.5777 |
[Training] epoch   6: [4500/14786] | RD Loss: 3.2763 | R:1.1023, D:70.9135 | Aux loss: 139.26 | Batch time: 0.5780 |
[Training] epoch   6: [5000/14786] | RD Loss: 3.2636 | R:1.1025, D:70.7411 | Aux loss: 143.59 | Batch time: 0.5780 |
[Training] epoch   6: [5500/14786] | RD Loss: 3.2524 | R:1.1028, D:70.5193 | Aux loss: 172.59 | Batch time: 0.5778 |
[Training] epoch   6: [6000/14786] | RD Loss: 3.2470 | R:1.1042, D:70.2723 | Aux loss: 160.92 | Batch time: 0.5779 |
[Training] epoch   6: [6500/14786] | RD Loss: 3.2397 | R:1.1046, D:69.9590 | Aux loss: 144.12 | Batch time: 0.5780 |
[Training] epoch   6: [7000/14786] | RD Loss: 3.2343 | R:1.1061, D:69.8717 | Aux loss: 163.68 | Batch time: 0.5778 |
[Evaluating] epoch   6: RD Loss: 2.5640 | best RD loss: 2.0511 | R loss: 0.6671 | D Loss: 67.5004 | Aux loss: 156.8518
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460    0.00600     0.00740     0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.393489    0.464936    0.530847    0.626245    0.729026   0.896360    1.098936    1.350542   1.568361   1.810612  ...   2.117796   2.165889   2.420813   2.830701   3.346195   3.945626   4.685132   5.666623   7.120783   9.148737
r_loss     0.159992    0.180065    0.203497    0.230886    0.262462   0.300321    0.343104    0.390273   0.442481   0.501412  ...   0.771149   0.844924   0.900291   0.957984   1.014931   1.076158   1.139448   1.211988   1.239973   1.275138
d_loss   129.720340  121.221729  112.879171  105.429058  101.426898  99.339730  102.139460  100.027952  95.413547  85.289898  ...  34.309493  27.349154  26.352200  27.909340  29.086267  30.788290  31.842693  34.398725  38.002003  43.742212

[3 rows x 23 columns]
[Training] epoch   6: [7500/14786] | RD Loss: 3.2317 | R:1.1061, D:69.8497 | Aux loss: 163.21 | Batch time: 0.5780 |
[Training] epoch   6: [8000/14786] | RD Loss: 3.2306 | R:1.1074, D:69.7952 | Aux loss: 141.23 | Batch time: 0.5781 |
[Training] epoch   6: [8500/14786] | RD Loss: 3.2219 | R:1.1073, D:69.6248 | Aux loss: 154.49 | Batch time: 0.5782 |
[Training] epoch   6: [9000/14786] | RD Loss: 3.2206 | R:1.1081, D:69.6128 | Aux loss: 154.62 | Batch time: 0.5782 |
[Training] epoch   6: [9500/14786] | RD Loss: 3.2194 | R:1.1080, D:69.5954 | Aux loss: 165.51 | Batch time: 0.5782 |
[Training] epoch   6: [10000/14786] | RD Loss: 3.2141 | R:1.1085, D:69.4118 | Aux loss: 142.56 | Batch time: 0.5782 |
[Training] epoch   6: [10500/14786] | RD Loss: 3.2075 | R:1.1097, D:69.3685 | Aux loss: 155.18 | Batch time: 0.5783 |
[Training] epoch   6: [11000/14786] | RD Loss: 3.2034 | R:1.1103, D:69.2862 | Aux loss: 165.15 | Batch time: 0.5784 |
[Training] epoch   6: [11500/14786] | RD Loss: 3.2019 | R:1.1113, D:69.2852 | Aux loss: 171.17 | Batch time: 0.5784 |
[Training] epoch   6: [12000/14786] | RD Loss: 3.2014 | R:1.1122, D:69.2349 | Aux loss: 150.51 | Batch time: 0.5785 |
[Training] epoch   6: [12500/14786] | RD Loss: 3.1977 | R:1.1121, D:69.3417 | Aux loss: 161.18 | Batch time: 0.5785 |
[Training] epoch   6: [13000/14786] | RD Loss: 3.1935 | R:1.1125, D:69.1972 | Aux loss: 170.84 | Batch time: 0.5783 |
[Training] epoch   6: [13500/14786] | RD Loss: 3.1901 | R:1.1126, D:69.2000 | Aux loss: 190.92 | Batch time: 0.5784 |
[Training] epoch   6: [14000/14786] | RD Loss: 3.1864 | R:1.1134, D:69.0990 | Aux loss: 157.64 | Batch time: 0.5784 |
[Training] epoch   6: [14500/14786] | RD Loss: 3.1819 | R:1.1132, D:69.0150 | Aux loss: 167.54 | Batch time: 0.5782 |
[Evaluating] epoch   6: RD Loss: 2.4265 | best RD loss: 2.0511 | R loss: 0.6833 | D Loss: 62.4770 | Aux loss: 146.3566
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.341001   0.396846   0.449621   0.532032   0.623405   0.753953   0.904340   1.142586   1.427905   1.746035  ...   3.745258   4.218617   4.133664   3.908627   3.702805   3.522893   3.546569   3.668037   4.208959   4.929370
r_loss    0.163369   0.185863   0.212209   0.242681   0.277309   0.315745   0.358592   0.405879   0.457672   0.512554  ...   0.766337   0.838992   0.899843   0.964074   1.034551   1.104485   1.173035   1.250506   1.295433   1.355591
d_loss   98.684565  89.779954  81.866356  77.160184  75.238341  73.034652  73.749637  76.740312  82.223080  80.357075  ...  75.896083  69.971528  56.045417  43.883049  33.290761  25.948588  21.315975  18.668193  18.827310  19.854323

[3 rows x 23 columns]
[training] In epoch 7, lr: 0.0001, Aux lr: 0.001
[Training] epoch   7: [   0/14786] | RD Loss: 0.9967 | R:0.6121, D:132.6218 | Aux loss: 152.05 | Batch time: 0.7425 |
[Training] epoch   7: [ 500/14786] | RD Loss: 3.1215 | R:1.1419, D:69.0075 | Aux loss: 166.13 | Batch time: 0.5941 |
[Training] epoch   7: [1000/14786] | RD Loss: 3.1108 | R:1.1339, D:68.4452 | Aux loss: 156.42 | Batch time: 0.6029 |
[Training] epoch   7: [1500/14786] | RD Loss: 3.0676 | R:1.1292, D:67.5372 | Aux loss: 183.04 | Batch time: 0.6046 |
[Training] epoch   7: [2000/14786] | RD Loss: 3.0789 | R:1.1302, D:67.3699 | Aux loss: 166.57 | Batch time: 0.6020 |
[Training] epoch   7: [2500/14786] | RD Loss: 3.0544 | R:1.1285, D:67.0606 | Aux loss: 161.45 | Batch time: 0.6004 |
[Training] epoch   7: [3000/14786] | RD Loss: 3.0461 | R:1.1284, D:66.7723 | Aux loss: 180.47 | Batch time: 0.5993 |
[Training] epoch   7: [3500/14786] | RD Loss: 3.0395 | R:1.1276, D:66.4678 | Aux loss: 168.85 | Batch time: 0.5978 |
[Training] epoch   7: [4000/14786] | RD Loss: 3.0327 | R:1.1270, D:66.1660 | Aux loss: 172.08 | Batch time: 0.5952 |
[Training] epoch   7: [4500/14786] | RD Loss: 3.0326 | R:1.1267, D:66.1794 | Aux loss: 161.13 | Batch time: 0.5935 |
[Training] epoch   7: [5000/14786] | RD Loss: 3.0127 | R:1.1249, D:65.6995 | Aux loss: 194.73 | Batch time: 0.5922 |
[Training] epoch   7: [5500/14786] | RD Loss: 3.0153 | R:1.1252, D:65.6866 | Aux loss: 178.59 | Batch time: 0.5915 |
[Training] epoch   7: [6000/14786] | RD Loss: 3.0178 | R:1.1265, D:65.7817 | Aux loss: 152.06 | Batch time: 0.5920 |
[Training] epoch   7: [6500/14786] | RD Loss: 3.0136 | R:1.1269, D:65.7092 | Aux loss: 166.02 | Batch time: 0.5911 |
[Training] epoch   7: [7000/14786] | RD Loss: 3.0263 | R:1.1276, D:66.0098 | Aux loss: 175.07 | Batch time: 0.5906 |
[Evaluating] epoch   7: RD Loss: 3.9272 | best RD loss: 2.0511 | R loss: 0.6822 | D Loss: 59.7299 | Aux loss: 159.4901
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.345644   0.399433   0.447702   0.503001   0.555766   0.637715   0.720192   0.824663   0.915064   1.038297  ...   1.839267   2.664242   3.551165   4.884103   5.963831   7.646196   9.861141  13.372334  14.421166  15.802317
r_loss     0.155876   0.176865   0.201038   0.228338   0.260121   0.297402   0.340182   0.386372   0.438050   0.492341  ...   0.766115   0.850218   0.915640   0.989864   1.056146   1.131965   1.200793   1.284883   1.310298   1.343938
d_loss   105.426564  94.709909  85.056601  73.243502  64.270490  56.718692  51.352644  45.655333  40.424924  35.567156  ...  27.341450  37.557424  45.676355  58.036346  61.231251  69.895186  77.775916  93.339385  84.722893  80.324320

[3 rows x 23 columns]
[Training] epoch   7: [7500/14786] | RD Loss: 3.0396 | R:1.1280, D:66.4389 | Aux loss: 154.31 | Batch time: 0.5901 |
[Training] epoch   7: [8000/14786] | RD Loss: 3.0384 | R:1.1281, D:66.2853 | Aux loss: 159.88 | Batch time: 0.5898 |
[Training] epoch   7: [8500/14786] | RD Loss: 3.0376 | R:1.1284, D:66.2427 | Aux loss: 171.42 | Batch time: 0.5891 |
[Training] epoch   7: [9000/14786] | RD Loss: 3.0365 | R:1.1288, D:66.2447 | Aux loss: 183.44 | Batch time: 0.5887 |
[Training] epoch   7: [9500/14786] | RD Loss: 3.0365 | R:1.1290, D:66.2107 | Aux loss: 173.81 | Batch time: 0.5888 |
[Training] epoch   7: [10000/14786] | RD Loss: 3.0376 | R:1.1293, D:66.3031 | Aux loss: 160.99 | Batch time: 0.5897 |
[Training] epoch   7: [10500/14786] | RD Loss: 3.0359 | R:1.1287, D:66.3647 | Aux loss: 175.90 | Batch time: 0.5914 |
[Training] epoch   7: [11000/14786] | RD Loss: 3.0392 | R:1.1291, D:66.4236 | Aux loss: 178.84 | Batch time: 0.5925 |
[Training] epoch   7: [11500/14786] | RD Loss: 3.0368 | R:1.1300, D:66.3022 | Aux loss: 176.22 | Batch time: 0.5939 |
[Training] epoch   7: [12000/14786] | RD Loss: 3.0324 | R:1.1301, D:66.1133 | Aux loss: 178.00 | Batch time: 0.5944 |
[Training] epoch   7: [12500/14786] | RD Loss: 3.0298 | R:1.1298, D:66.1307 | Aux loss: 176.32 | Batch time: 0.5955 |
[Training] epoch   7: [13000/14786] | RD Loss: 3.0335 | R:1.1305, D:66.1944 | Aux loss: 168.32 | Batch time: 0.5964 |
[Training] epoch   7: [13500/14786] | RD Loss: 3.0358 | R:1.1305, D:66.2318 | Aux loss: 163.73 | Batch time: 0.5957 |
[Training] epoch   7: [14000/14786] | RD Loss: 3.0309 | R:1.1299, D:66.1586 | Aux loss: 182.42 | Batch time: 0.5953 |
[Training] epoch   7: [14500/14786] | RD Loss: 3.0302 | R:1.1302, D:66.1262 | Aux loss: 191.07 | Batch time: 0.5948 |
[Evaluating] epoch   7: RD Loss: 2.5140 | best RD loss: 2.0511 | R loss: 0.7107 | D Loss: 64.3768 | Aux loss: 173.6170
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.344110   0.401601   0.456943   0.539509   0.627423   0.760934   0.904939   1.120478   1.362620   1.736201  ...   3.895544   4.481240   4.712427   4.770079   4.320401   3.865531   3.657661   3.484262   3.908002   4.412191
r_loss     0.159545   0.181401   0.206367   0.236159   0.270276   0.309546   0.354884   0.406076   0.464087   0.526003  ...   0.824317   0.909834   0.969247   1.033031   1.095098   1.158572   1.222143   1.290897   1.334900   1.386108
d_loss   102.536400  93.702209  86.405202  80.893404  77.640684  75.231356  74.331681  74.416847  76.146850  78.840216  ...  78.247836  73.942163  64.873143  55.693702  40.240842  29.044625  21.872636  16.937178  16.627474  16.811569

[3 rows x 23 columns]
[training] In epoch 8, lr: 0.0001, Aux lr: 0.001
[Training] epoch   8: [   0/14786] | RD Loss: 5.9718 | R:1.7136, D:23.6564 | Aux loss: 174.14 | Batch time: 0.7746 |
[Training] epoch   8: [ 500/14786] | RD Loss: 3.0728 | R:1.1318, D:67.6180 | Aux loss: 167.08 | Batch time: 0.5773 |
[Training] epoch   8: [1000/14786] | RD Loss: 3.0252 | R:1.1328, D:66.6805 | Aux loss: 176.05 | Batch time: 0.5757 |
[Training] epoch   8: [1500/14786] | RD Loss: 3.0370 | R:1.1325, D:66.1827 | Aux loss: 184.60 | Batch time: 0.5770 |
[Training] epoch   8: [2000/14786] | RD Loss: 3.0466 | R:1.1374, D:65.9029 | Aux loss: 171.48 | Batch time: 0.5777 |
[Training] epoch   8: [2500/14786] | RD Loss: 3.0432 | R:1.1392, D:66.2888 | Aux loss: 168.24 | Batch time: 0.5773 |
[Training] epoch   8: [3000/14786] | RD Loss: 3.0289 | R:1.1400, D:66.0033 | Aux loss: 174.31 | Batch time: 0.5775 |
[Training] epoch   8: [3500/14786] | RD Loss: 3.0400 | R:1.1430, D:66.0702 | Aux loss: 178.17 | Batch time: 0.5779 |
[Training] epoch   8: [4000/14786] | RD Loss: 3.0164 | R:1.1428, D:65.5090 | Aux loss: 164.14 | Batch time: 0.5777 |
[Training] epoch   8: [4500/14786] | RD Loss: 3.0054 | R:1.1400, D:65.3506 | Aux loss: 177.72 | Batch time: 0.5777 |
[Training] epoch   8: [5000/14786] | RD Loss: 2.9947 | R:1.1378, D:65.1886 | Aux loss: 183.02 | Batch time: 0.5781 |
[Training] epoch   8: [5500/14786] | RD Loss: 2.9896 | R:1.1387, D:65.0071 | Aux loss: 183.82 | Batch time: 0.5785 |
[Training] epoch   8: [6000/14786] | RD Loss: 2.9822 | R:1.1385, D:64.8875 | Aux loss: 173.87 | Batch time: 0.5785 |
[Training] epoch   8: [6500/14786] | RD Loss: 2.9695 | R:1.1372, D:64.6113 | Aux loss: 180.10 | Batch time: 0.5785 |
[Training] epoch   8: [7000/14786] | RD Loss: 2.9748 | R:1.1388, D:64.7237 | Aux loss: 167.37 | Batch time: 0.5784 |
[Evaluating] epoch   8: RD Loss: 2.4882 | best RD loss: 2.0511 | R loss: 0.6943 | D Loss: 47.3984 | Aux loss: 207.0208
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.321307   0.373645   0.422096   0.484309   0.539483   0.616318   0.683389   0.781162   0.869087   1.009281  ...   1.994827   2.571877   3.352394   4.291706   4.877992   5.361555   5.638837   5.918139   6.286977   6.753261
r_loss    0.146473   0.167359   0.191919   0.218882   0.249963   0.287124   0.329521   0.377218   0.429864   0.489545  ...   0.776412   0.863327   0.934379   1.009505   1.083430   1.160300   1.237467   1.321825   1.377307   1.443770
d_loss   97.130033  87.781385  79.371292  70.780363  62.939061  54.865688  47.819992  42.077555  37.222272  33.858978  ...  31.042402  35.373700  41.906659  48.915063  47.343262  45.077840  39.527351  35.492775  31.726458  29.497170

[3 rows x 23 columns]
[Training] epoch   8: [7500/14786] | RD Loss: 2.9763 | R:1.1393, D:64.6603 | Aux loss: 191.58 | Batch time: 0.5788 |
[Training] epoch   8: [8000/14786] | RD Loss: 2.9852 | R:1.1396, D:64.9881 | Aux loss: 183.83 | Batch time: 0.5788 |
[Training] epoch   8: [8500/14786] | RD Loss: 2.9861 | R:1.1401, D:65.0843 | Aux loss: 213.50 | Batch time: 0.5789 |
[Training] epoch   8: [9000/14786] | RD Loss: 2.9827 | R:1.1403, D:64.9908 | Aux loss: 203.11 | Batch time: 0.5788 |
[Training] epoch   8: [9500/14786] | RD Loss: 2.9806 | R:1.1409, D:64.8534 | Aux loss: 165.46 | Batch time: 0.5788 |
[Training] epoch   8: [10000/14786] | RD Loss: 2.9817 | R:1.1408, D:64.8940 | Aux loss: 176.93 | Batch time: 0.5787 |
[Training] epoch   8: [10500/14786] | RD Loss: 2.9815 | R:1.1407, D:64.8833 | Aux loss: 185.87 | Batch time: 0.5786 |
[Training] epoch   8: [11000/14786] | RD Loss: 2.9832 | R:1.1409, D:64.9194 | Aux loss: 220.87 | Batch time: 0.5786 |
[Training] epoch   8: [11500/14786] | RD Loss: 2.9855 | R:1.1413, D:64.9929 | Aux loss: 169.93 | Batch time: 0.5786 |
[Training] epoch   8: [12000/14786] | RD Loss: 2.9824 | R:1.1411, D:64.9794 | Aux loss: 180.84 | Batch time: 0.5785 |
[Training] epoch   8: [12500/14786] | RD Loss: 2.9784 | R:1.1409, D:64.9009 | Aux loss: 198.52 | Batch time: 0.5785 |
[Training] epoch   8: [13000/14786] | RD Loss: 2.9788 | R:1.1412, D:64.8754 | Aux loss: 196.24 | Batch time: 0.5784 |
[Training] epoch   8: [13500/14786] | RD Loss: 2.9748 | R:1.1414, D:64.7657 | Aux loss: 177.66 | Batch time: 0.5784 |
[Training] epoch   8: [14000/14786] | RD Loss: 2.9789 | R:1.1411, D:64.8941 | Aux loss: 167.32 | Batch time: 0.5785 |
[Training] epoch   8: [14500/14786] | RD Loss: 2.9767 | R:1.1414, D:64.8417 | Aux loss: 211.61 | Batch time: 0.5785 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch   8: RD Loss: 1.6290 | best RD loss: 1.6290 | R loss: 0.7088 | D Loss: 43.6245 | Aux loss: 182.9022
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.316498   0.370103   0.421202   0.503145   0.610856   0.757868   0.929424   1.080220   1.208096   1.284181  ...   1.619885   1.813338   1.959677   2.096713   2.291838   2.485546   2.727161   3.022626   3.524645   4.325829
r_loss    0.146095   0.167415   0.191965   0.220971   0.254506   0.294237   0.340524   0.393723   0.453619   0.518872  ...   0.814616   0.896319   0.958581   1.025778   1.093034   1.164951   1.238330   1.319853   1.375141   1.439397
d_loss   94.668363  86.250125  79.047324  75.246311  77.467285  77.271976  79.581041  71.510167  63.938745  49.857242  ...  20.516399  18.985896  17.350024  15.960274  14.956998  14.169479  13.370727  13.148828  13.890172  16.035733

[3 rows x 23 columns]
[training] In epoch 9, lr: 0.0001, Aux lr: 0.001
[Training] epoch   9: [   0/14786] | RD Loss: 4.9710 | R:1.8984, D:27.5942 | Aux loss: 180.26 | Batch time: 0.6717 |
[Training] epoch   9: [ 500/14786] | RD Loss: 2.8478 | R:1.1484, D:61.8491 | Aux loss: 176.34 | Batch time: 0.5999 |
[Training] epoch   9: [1000/14786] | RD Loss: 2.8180 | R:1.1475, D:60.8321 | Aux loss: 188.35 | Batch time: 0.5991 |
[Training] epoch   9: [1500/14786] | RD Loss: 2.8434 | R:1.1463, D:61.0766 | Aux loss: 198.27 | Batch time: 0.5957 |
[Training] epoch   9: [2000/14786] | RD Loss: 2.9008 | R:1.1488, D:62.4802 | Aux loss: 172.92 | Batch time: 0.5936 |
[Training] epoch   9: [2500/14786] | RD Loss: 2.9066 | R:1.1473, D:62.5114 | Aux loss: 186.20 | Batch time: 0.5929 |
[Training] epoch   9: [3000/14786] | RD Loss: 2.9198 | R:1.1501, D:62.8055 | Aux loss: 201.42 | Batch time: 0.5915 |
[Training] epoch   9: [3500/14786] | RD Loss: 2.9380 | R:1.1518, D:63.0973 | Aux loss: 176.80 | Batch time: 0.5898 |
[Training] epoch   9: [4000/14786] | RD Loss: 2.9335 | R:1.1503, D:63.0952 | Aux loss: 195.92 | Batch time: 0.5890 |
[Training] epoch   9: [4500/14786] | RD Loss: 2.9329 | R:1.1499, D:63.5012 | Aux loss: 211.70 | Batch time: 0.5881 |
[Training] epoch   9: [5000/14786] | RD Loss: 2.9406 | R:1.1518, D:63.6123 | Aux loss: 196.03 | Batch time: 0.5874 |
[Training] epoch   9: [5500/14786] | RD Loss: 2.9354 | R:1.1533, D:63.2893 | Aux loss: 183.61 | Batch time: 0.5873 |
[Training] epoch   9: [6000/14786] | RD Loss: 2.9315 | R:1.1528, D:63.3124 | Aux loss: 184.76 | Batch time: 0.5869 |
[Training] epoch   9: [6500/14786] | RD Loss: 2.9325 | R:1.1526, D:63.3749 | Aux loss: 192.66 | Batch time: 0.5878 |
[Training] epoch   9: [7000/14786] | RD Loss: 2.9305 | R:1.1523, D:63.1915 | Aux loss: 237.36 | Batch time: 0.5877 |
[Evaluating] epoch   9: RD Loss: 2.9874 | best RD loss: 1.6290 | R loss: 0.7094 | D Loss: 62.3156 | Aux loss: 213.1678
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.317056   0.369425   0.419166   0.479050   0.532348   0.612283   0.689473   0.828857   1.025086   1.381142  ...   4.393324   5.245677   5.360517   5.340854   5.131748   5.067095   5.232825   5.586358   6.096545   6.639668
r_loss    0.140322   0.161029   0.185340   0.214721   0.248861   0.287838   0.332576   0.384152   0.441698   0.507308  ...   0.827725   0.919802   0.981046   1.046951   1.107580   1.172504   1.245220   1.323201   1.375441   1.431936
d_loss   98.185110  88.679001  80.629706  70.487798  61.627752  54.074063  48.229455  46.323483  49.439720  56.927328  ...  90.843286  89.562613  75.900706  63.992588  50.207953  41.787450  35.811451  32.920127  30.507940  28.931845

[3 rows x 23 columns]
[Training] epoch   9: [7500/14786] | RD Loss: 2.9380 | R:1.1536, D:63.1946 | Aux loss: 192.63 | Batch time: 0.5873 |
[Training] epoch   9: [8000/14786] | RD Loss: 2.9387 | R:1.1531, D:63.4107 | Aux loss: 187.48 | Batch time: 0.5869 |
[Training] epoch   9: [8500/14786] | RD Loss: 2.9329 | R:1.1533, D:63.3373 | Aux loss: 183.00 | Batch time: 0.5866 |
[Training] epoch   9: [9000/14786] | RD Loss: 2.9332 | R:1.1534, D:63.3788 | Aux loss: 188.72 | Batch time: 0.5865 |
[Training] epoch   9: [9500/14786] | RD Loss: 2.9330 | R:1.1536, D:63.2912 | Aux loss: 185.29 | Batch time: 0.5861 |
[Training] epoch   9: [10000/14786] | RD Loss: 2.9349 | R:1.1538, D:63.3133 | Aux loss: 191.29 | Batch time: 0.5859 |
[Training] epoch   9: [10500/14786] | RD Loss: 2.9312 | R:1.1538, D:63.2293 | Aux loss: 189.89 | Batch time: 0.5859 |
[Training] epoch   9: [11000/14786] | RD Loss: 2.9281 | R:1.1545, D:63.1487 | Aux loss: 174.87 | Batch time: 0.5857 |
[Training] epoch   9: [11500/14786] | RD Loss: 2.9248 | R:1.1545, D:63.1150 | Aux loss: 191.76 | Batch time: 0.5856 |
[Training] epoch   9: [12000/14786] | RD Loss: 2.9219 | R:1.1537, D:63.0947 | Aux loss: 199.86 | Batch time: 0.5856 |
[Training] epoch   9: [12500/14786] | RD Loss: 2.9217 | R:1.1541, D:63.0021 | Aux loss: 217.35 | Batch time: 0.5859 |
[Training] epoch   9: [13000/14786] | RD Loss: 2.9183 | R:1.1542, D:62.9379 | Aux loss: 193.71 | Batch time: 0.5862 |
[Training] epoch   9: [13500/14786] | RD Loss: 2.9186 | R:1.1541, D:62.9416 | Aux loss: 190.12 | Batch time: 0.5864 |
[Training] epoch   9: [14000/14786] | RD Loss: 2.9170 | R:1.1536, D:62.9308 | Aux loss: 195.39 | Batch time: 0.5867 |
[Training] epoch   9: [14500/14786] | RD Loss: 2.9174 | R:1.1540, D:62.9443 | Aux loss: 206.04 | Batch time: 0.5871 |
[Evaluating] epoch   9: RD Loss: 1.8038 | best RD loss: 1.6290 | R loss: 0.7033 | D Loss: 42.8181 | Aux loss: 192.5802
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.330959   0.386549   0.437772   0.507947   0.576994   0.669007   0.752578   0.862510   0.968563   1.109515  ...   1.712758   1.860030   1.984611   2.142895   2.406177   2.794916   3.305094   4.020684   4.791546   5.708545
r_loss     0.142589   0.164136   0.188431   0.215984   0.247795   0.284792   0.326560   0.373720   0.425947   0.483955  ...   0.790942   0.886329   0.960712   1.037524   1.110814   1.188453   1.266022   1.349095   1.404087   1.456755
d_loss   104.649969  94.643776  85.979583  77.856793  71.565007  64.035728  57.570020  50.915714  45.984434  40.753080  ...  23.485753  20.159452  17.745215  16.473485  16.161737  17.236722  18.312273  20.630035  21.889876  23.621055

[3 rows x 23 columns]
[training] In epoch 10, lr: 0.0001, Aux lr: 0.001
[Training] epoch  10: [   0/14786] | RD Loss: 3.5540 | R:1.5737, D:29.5135 | Aux loss: 188.46 | Batch time: 0.8123 |
[Training] epoch  10: [ 500/14786] | RD Loss: 2.9569 | R:1.1627, D:63.1904 | Aux loss: 215.03 | Batch time: 0.5948 |
[Training] epoch  10: [1000/14786] | RD Loss: 2.8959 | R:1.1546, D:62.2801 | Aux loss: 203.08 | Batch time: 0.5957 |
[Training] epoch  10: [1500/14786] | RD Loss: 2.8648 | R:1.1540, D:61.6093 | Aux loss: 187.21 | Batch time: 0.5951 |
[Training] epoch  10: [2000/14786] | RD Loss: 2.8664 | R:1.1482, D:61.6394 | Aux loss: 193.93 | Batch time: 0.5934 |
[Training] epoch  10: [2500/14786] | RD Loss: 2.8723 | R:1.1509, D:62.1135 | Aux loss: 229.40 | Batch time: 0.5936 |
[Training] epoch  10: [3000/14786] | RD Loss: 2.8713 | R:1.1529, D:62.0077 | Aux loss: 181.90 | Batch time: 0.5940 |
[Training] epoch  10: [3500/14786] | RD Loss: 2.8650 | R:1.1540, D:61.9827 | Aux loss: 180.53 | Batch time: 0.5938 |
[Training] epoch  10: [4000/14786] | RD Loss: 2.8736 | R:1.1553, D:62.1511 | Aux loss: 179.69 | Batch time: 0.5944 |
[Training] epoch  10: [4500/14786] | RD Loss: 2.8680 | R:1.1551, D:62.0304 | Aux loss: 213.74 | Batch time: 0.5957 |
[Training] epoch  10: [5000/14786] | RD Loss: 2.8683 | R:1.1548, D:61.9910 | Aux loss: 217.09 | Batch time: 0.5991 |
[Training] epoch  10: [5500/14786] | RD Loss: 2.8680 | R:1.1556, D:61.9482 | Aux loss: 194.79 | Batch time: 0.6016 |
[Training] epoch  10: [6000/14786] | RD Loss: 2.8686 | R:1.1553, D:62.0341 | Aux loss: 180.42 | Batch time: 0.6045 |
[Training] epoch  10: [6500/14786] | RD Loss: 2.8575 | R:1.1543, D:61.8181 | Aux loss: 201.93 | Batch time: 0.6077 |
[Training] epoch  10: [7000/14786] | RD Loss: 2.8615 | R:1.1543, D:61.8580 | Aux loss: 192.96 | Batch time: 0.6098 |
[Evaluating] epoch  10: RD Loss: 2.1799 | best RD loss: 1.6290 | R loss: 0.7151 | D Loss: 44.2351 | Aux loss: 190.6776
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.311775   0.362572   0.410424   0.476844   0.542273   0.636920   0.727889   0.843092   0.950860   1.105033  ...   1.973810   2.317910   2.631955   2.993478   3.395256   3.891046   4.479607   5.226475   5.974330   6.514301
r_loss    0.144654   0.166420   0.191563   0.219482   0.251752   0.289987   0.334127   0.382055   0.437094   0.496066  ...   0.800008   0.893796   0.967240   1.046983   1.122781   1.205298   1.284596   1.373999   1.431522   1.495946
d_loss   92.844863  83.468993  75.469381  68.629699  63.156580  57.822175  53.211142  48.024626  43.539456  39.672107  ...  29.905778  29.484773  28.851212  29.008860  28.352774  28.817035  28.693403  29.748848  29.355786  27.879747

[3 rows x 23 columns]
[Training] epoch  10: [7500/14786] | RD Loss: 2.8562 | R:1.1543, D:61.7189 | Aux loss: 211.54 | Batch time: 0.6096 |
[Training] epoch  10: [8000/14786] | RD Loss: 2.8484 | R:1.1536, D:61.5773 | Aux loss: 217.17 | Batch time: 0.6102 |
[Training] epoch  10: [8500/14786] | RD Loss: 2.8470 | R:1.1535, D:61.5461 | Aux loss: 195.70 | Batch time: 0.6118 |
[Training] epoch  10: [9000/14786] | RD Loss: 2.8434 | R:1.1524, D:61.5029 | Aux loss: 189.46 | Batch time: 0.6132 |
[Training] epoch  10: [9500/14786] | RD Loss: 2.8402 | R:1.1526, D:61.5230 | Aux loss: 218.04 | Batch time: 0.6153 |
[Training] epoch  10: [10000/14786] | RD Loss: 2.8384 | R:1.1522, D:61.5724 | Aux loss: 213.79 | Batch time: 0.6159 |
[Training] epoch  10: [10500/14786] | RD Loss: 2.8409 | R:1.1522, D:61.5713 | Aux loss: 200.45 | Batch time: 0.6162 |
[Training] epoch  10: [11000/14786] | RD Loss: 2.8366 | R:1.1519, D:61.5273 | Aux loss: 189.85 | Batch time: 0.6154 |
[Training] epoch  10: [11500/14786] | RD Loss: 2.8353 | R:1.1516, D:61.5577 | Aux loss: 184.63 | Batch time: 0.6139 |
[Training] epoch  10: [12000/14786] | RD Loss: 2.8329 | R:1.1517, D:61.5439 | Aux loss: 193.95 | Batch time: 0.6126 |
[Training] epoch  10: [12500/14786] | RD Loss: 2.8320 | R:1.1517, D:61.5097 | Aux loss: 217.51 | Batch time: 0.6115 |
[Training] epoch  10: [13000/14786] | RD Loss: 2.8306 | R:1.1515, D:61.4797 | Aux loss: 184.44 | Batch time: 0.6102 |
[Training] epoch  10: [13500/14786] | RD Loss: 2.8287 | R:1.1510, D:61.5346 | Aux loss: 187.98 | Batch time: 0.6091 |
[Training] epoch  10: [14000/14786] | RD Loss: 2.8253 | R:1.1504, D:61.5073 | Aux loss: 212.24 | Batch time: 0.6080 |
[Training] epoch  10: [14500/14786] | RD Loss: 2.8245 | R:1.1503, D:61.5054 | Aux loss: 215.57 | Batch time: 0.6071 |
[Evaluating] epoch  10: RD Loss: 1.8454 | best RD loss: 1.6290 | R loss: 0.6813 | D Loss: 45.2920 | Aux loss: 200.7160
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.345914    0.408216   0.462502   0.534615   0.598406   0.685053   0.755488   0.851305   0.932656   1.044691  ...   1.632998   1.855507   2.069350   2.295622   2.573175   2.934521   3.442560   4.184165   5.010375   5.969111
r_loss     0.135926    0.156510   0.180418   0.207339   0.238453   0.274256   0.315513   0.362317   0.414661   0.472350  ...   0.766731   0.856828   0.925175   0.998439   1.072783   1.153554   1.230297   1.315653   1.358725   1.405473
d_loss   116.660133  107.108683  97.270284  87.273610  78.250494  68.466157  59.456157  50.936206  43.897811  37.286059  ...  22.070500  20.676585  19.829730  19.332096  18.719799  19.109091  19.867659  22.150673  23.597092  25.353548

[3 rows x 23 columns]
[training] In epoch 11, lr: 0.0001, Aux lr: 0.001
[Training] epoch  11: [   0/14786] | RD Loss: 2.7556 | R:1.2587, D:38.1354 | Aux loss: 196.14 | Batch time: 0.7882 |
[Training] epoch  11: [ 500/14786] | RD Loss: 2.8687 | R:1.1591, D:63.1160 | Aux loss: 217.86 | Batch time: 0.5886 |
[Training] epoch  11: [1000/14786] | RD Loss: 2.8745 | R:1.1579, D:62.9552 | Aux loss: 199.89 | Batch time: 0.5977 |
[Training] epoch  11: [1500/14786] | RD Loss: 2.8361 | R:1.1482, D:62.1014 | Aux loss: 191.21 | Batch time: 0.6045 |
[Training] epoch  11: [2000/14786] | RD Loss: 2.8209 | R:1.1457, D:61.7222 | Aux loss: 220.08 | Batch time: 0.6070 |
[Training] epoch  11: [2500/14786] | RD Loss: 2.8232 | R:1.1445, D:61.4666 | Aux loss: 201.49 | Batch time: 0.6104 |
[Training] epoch  11: [3000/14786] | RD Loss: 2.8034 | R:1.1434, D:60.8054 | Aux loss: 195.65 | Batch time: 0.6054 |
[Training] epoch  11: [3500/14786] | RD Loss: 2.8111 | R:1.1411, D:61.1949 | Aux loss: 192.77 | Batch time: 0.6020 |
[Training] epoch  11: [4000/14786] | RD Loss: 2.8133 | R:1.1413, D:61.2716 | Aux loss: 209.05 | Batch time: 0.5992 |
[Training] epoch  11: [4500/14786] | RD Loss: 2.7959 | R:1.1401, D:61.0051 | Aux loss: 198.67 | Batch time: 0.5990 |
[Training] epoch  11: [5000/14786] | RD Loss: 2.7961 | R:1.1408, D:61.0251 | Aux loss: 184.09 | Batch time: 0.6007 |
[Training] epoch  11: [5500/14786] | RD Loss: 2.7888 | R:1.1407, D:60.9540 | Aux loss: 209.12 | Batch time: 0.6021 |
[Training] epoch  11: [6000/14786] | RD Loss: 2.7898 | R:1.1404, D:61.0002 | Aux loss: 227.44 | Batch time: 0.6026 |
[Training] epoch  11: [6500/14786] | RD Loss: 2.7903 | R:1.1410, D:61.0029 | Aux loss: 235.32 | Batch time: 0.6032 |
[Training] epoch  11: [7000/14786] | RD Loss: 2.7889 | R:1.1421, D:60.8535 | Aux loss: 203.12 | Batch time: 0.6033 |
[Evaluating] epoch  11: RD Loss: 2.1825 | best RD loss: 1.6290 | R loss: 0.7031 | D Loss: 62.4034 | Aux loss: 194.2429
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.341545    0.401654   0.460817   0.542721   0.634934   0.777390   0.943675   1.170145   1.431184   1.762788  ...   3.097015   3.364083   3.207476   3.017472   2.781183   2.890757   3.439144   4.456672   4.219418   4.021059
r_loss     0.135672    0.155539   0.178331   0.203725   0.233330   0.269582   0.311930   0.360396   0.415719   0.475680  ...   0.769577   0.858519   0.931558   1.011259   1.095144   1.191359   1.295585   1.417692   1.478707   1.539196
d_loss   114.374072  104.729747  97.409059  90.399023  87.305180  84.634583  85.370848  84.348883  86.056365  83.850700  ...  59.297789  51.875023  39.443981  29.898844  21.036038  18.233890  19.250636  23.467030  17.710569  13.788130

[3 rows x 23 columns]
[Training] epoch  11: [7500/14786] | RD Loss: 2.7803 | R:1.1413, D:60.7591 | Aux loss: 206.49 | Batch time: 0.6039 |
[Training] epoch  11: [8000/14786] | RD Loss: 2.7789 | R:1.1405, D:60.6894 | Aux loss: 228.36 | Batch time: 0.6043 |
[Training] epoch  11: [8500/14786] | RD Loss: 2.7806 | R:1.1412, D:60.5674 | Aux loss: 202.41 | Batch time: 0.6046 |
[Training] epoch  11: [9000/14786] | RD Loss: 2.7796 | R:1.1409, D:60.5174 | Aux loss: 207.18 | Batch time: 0.6050 |
[Training] epoch  11: [9500/14786] | RD Loss: 2.7775 | R:1.1408, D:60.5035 | Aux loss: 204.90 | Batch time: 0.6053 |
[Training] epoch  11: [10000/14786] | RD Loss: 2.7741 | R:1.1405, D:60.4724 | Aux loss: 210.89 | Batch time: 0.6055 |
[Training] epoch  11: [10500/14786] | RD Loss: 2.7744 | R:1.1410, D:60.5141 | Aux loss: 227.22 | Batch time: 0.6057 |
[Training] epoch  11: [11000/14786] | RD Loss: 2.7737 | R:1.1410, D:60.5272 | Aux loss: 194.53 | Batch time: 0.6054 |
[Training] epoch  11: [11500/14786] | RD Loss: 2.7743 | R:1.1409, D:60.5863 | Aux loss: 208.04 | Batch time: 0.6054 |
[Training] epoch  11: [12000/14786] | RD Loss: 2.7705 | R:1.1407, D:60.5799 | Aux loss: 222.23 | Batch time: 0.6068 |
[Training] epoch  11: [12500/14786] | RD Loss: 2.7706 | R:1.1407, D:60.5907 | Aux loss: 206.84 | Batch time: 0.6071 |
[Training] epoch  11: [13000/14786] | RD Loss: 2.7694 | R:1.1408, D:60.6140 | Aux loss: 209.26 | Batch time: 0.6067 |
[Training] epoch  11: [13500/14786] | RD Loss: 2.7669 | R:1.1402, D:60.5901 | Aux loss: 222.20 | Batch time: 0.6065 |
[Training] epoch  11: [14000/14786] | RD Loss: 2.7658 | R:1.1403, D:60.5933 | Aux loss: 252.47 | Batch time: 0.6069 |
[Training] epoch  11: [14500/14786] | RD Loss: 2.7677 | R:1.1405, D:60.5666 | Aux loss: 199.71 | Batch time: 0.6072 |
[Evaluating] epoch  11: RD Loss: 1.6384 | best RD loss: 1.6290 | R loss: 0.6731 | D Loss: 37.8932 | Aux loss: 212.4349
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.308935   0.360390   0.404902   0.463090   0.514877   0.588698   0.654981   0.751319   0.839627   0.972095  ...   1.675335   1.867514   2.043982   2.214127   2.448446   2.692187   3.023925   3.420023   3.984839   4.624453
r_loss    0.134970   0.155875   0.180053   0.206357   0.236965   0.271142   0.310223   0.353467   0.401977   0.457310  ...   0.739727   0.827170   0.894013   0.966175   1.046143   1.132938   1.223165   1.324585   1.388202   1.463478
d_loss   96.647386  87.027926  77.534207  68.462279  60.415599  52.925919  46.588952  41.442943  37.088994  33.536495  ...  23.837140  21.539230  19.930134  18.598384  17.495970  16.730145  16.172071  16.180991  16.779564  17.560973

[3 rows x 23 columns]
[training] In epoch 12, lr: 0.0001, Aux lr: 0.001
[Training] epoch  12: [   0/14786] | RD Loss: 1.9522 | R:1.0565, D:47.3944 | Aux loss: 210.26 | Batch time: 0.7776 |
[Training] epoch  12: [ 500/14786] | RD Loss: 2.7527 | R:1.1476, D:60.5939 | Aux loss: 237.87 | Batch time: 0.5869 |
[Training] epoch  12: [1000/14786] | RD Loss: 2.7608 | R:1.1454, D:60.6897 | Aux loss: 196.48 | Batch time: 0.5950 |
[Training] epoch  12: [1500/14786] | RD Loss: 2.7534 | R:1.1397, D:60.7100 | Aux loss: 213.36 | Batch time: 0.6064 |
[Training] epoch  12: [2000/14786] | RD Loss: 2.7335 | R:1.1340, D:60.3714 | Aux loss: 200.75 | Batch time: 0.6152 |
[Training] epoch  12: [2500/14786] | RD Loss: 2.7341 | R:1.1365, D:60.0251 | Aux loss: 233.56 | Batch time: 0.6175 |
[Training] epoch  12: [3000/14786] | RD Loss: 2.7373 | R:1.1386, D:59.8891 | Aux loss: 197.74 | Batch time: 0.6183 |
[Training] epoch  12: [3500/14786] | RD Loss: 2.7325 | R:1.1390, D:59.9230 | Aux loss: 208.09 | Batch time: 0.6176 |
[Training] epoch  12: [4000/14786] | RD Loss: 2.7345 | R:1.1375, D:59.9740 | Aux loss: 214.40 | Batch time: 0.6192 |
[Training] epoch  12: [4500/14786] | RD Loss: 2.7370 | R:1.1382, D:60.0341 | Aux loss: 211.19 | Batch time: 0.6195 |
[Training] epoch  12: [5000/14786] | RD Loss: 2.7306 | R:1.1383, D:59.9169 | Aux loss: 209.64 | Batch time: 0.6190 |
[Training] epoch  12: [5500/14786] | RD Loss: 2.7202 | R:1.1376, D:59.7942 | Aux loss: 217.19 | Batch time: 0.6179 |
[Training] epoch  12: [6000/14786] | RD Loss: 2.7215 | R:1.1379, D:59.8763 | Aux loss: 220.57 | Batch time: 0.6185 |
[Training] epoch  12: [6500/14786] | RD Loss: 2.7259 | R:1.1385, D:59.8445 | Aux loss: 217.80 | Batch time: 0.6214 |
[Training] epoch  12: [7000/14786] | RD Loss: 2.7282 | R:1.1382, D:59.8852 | Aux loss: 207.07 | Batch time: 0.6238 |
[Evaluating] epoch  12: RD Loss: 1.8584 | best RD loss: 1.6290 | R loss: 0.6694 | D Loss: 77.2135 | Aux loss: 210.5203
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740     0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.484266    0.598542    0.718161    0.851928    0.973812    1.117629    1.242548    1.372262   1.452415   1.553475  ...   1.806132   1.973735   2.136556   2.273589   2.425088   2.561304   2.803979   3.053456   3.562320   4.843376
r_loss     0.131580    0.150493    0.171934    0.196353    0.224621    0.256984    0.293990    0.336855   0.384755   0.439211  ...   0.728241   0.826227   0.902502   0.984992   1.063616   1.150549   1.237913   1.338496   1.398931   1.469741
d_loss   195.936195  190.658783  188.354176  174.820008  162.867718  143.440798  128.183450  107.854928  90.479666  72.590481  ...  27.462180  23.757926  21.387416  19.204132  16.986544  15.136858  14.064363  13.242928  13.979902  18.742418

[3 rows x 23 columns]
[Training] epoch  12: [7500/14786] | RD Loss: 2.7265 | R:1.1378, D:60.0579 | Aux loss: 220.67 | Batch time: 0.6241 |
[Training] epoch  12: [8000/14786] | RD Loss: 2.7221 | R:1.1377, D:59.9768 | Aux loss: 255.06 | Batch time: 0.6252 |
[Training] epoch  12: [8500/14786] | RD Loss: 2.7189 | R:1.1377, D:59.8459 | Aux loss: 202.72 | Batch time: 0.6257 |
[Training] epoch  12: [9000/14786] | RD Loss: 2.7142 | R:1.1377, D:59.7564 | Aux loss: 200.96 | Batch time: 0.6255 |
[Training] epoch  12: [9500/14786] | RD Loss: 2.7087 | R:1.1373, D:59.7116 | Aux loss: 230.22 | Batch time: 0.6245 |
[Training] epoch  12: [10000/14786] | RD Loss: 2.7096 | R:1.1374, D:59.7193 | Aux loss: 215.38 | Batch time: 0.6224 |
[Training] epoch  12: [10500/14786] | RD Loss: 2.7061 | R:1.1378, D:59.5109 | Aux loss: 197.38 | Batch time: 0.6202 |
[Training] epoch  12: [11000/14786] | RD Loss: 2.7023 | R:1.1373, D:59.4205 | Aux loss: 207.31 | Batch time: 0.6181 |
[Training] epoch  12: [11500/14786] | RD Loss: 2.6997 | R:1.1368, D:59.3642 | Aux loss: 234.11 | Batch time: 0.6164 |
[Training] epoch  12: [12000/14786] | RD Loss: 2.7040 | R:1.1378, D:59.4708 | Aux loss: 215.94 | Batch time: 0.6148 |
[Training] epoch  12: [12500/14786] | RD Loss: 2.7052 | R:1.1381, D:59.4693 | Aux loss: 210.31 | Batch time: 0.6133 |
[Training] epoch  12: [13000/14786] | RD Loss: 2.7031 | R:1.1379, D:59.4411 | Aux loss: 208.97 | Batch time: 0.6119 |
[Training] epoch  12: [13500/14786] | RD Loss: 2.7025 | R:1.1379, D:59.4474 | Aux loss: 227.52 | Batch time: 0.6107 |
[Training] epoch  12: [14000/14786] | RD Loss: 2.6995 | R:1.1377, D:59.4012 | Aux loss: 222.70 | Batch time: 0.6095 |
[Training] epoch  12: [14500/14786] | RD Loss: 2.6986 | R:1.1377, D:59.3929 | Aux loss: 187.05 | Batch time: 0.6084 |
[Evaluating] epoch  12: RD Loss: 3.5071 | best RD loss: 1.6290 | R loss: 1.0692 | D Loss: 58.0892 | Aux loss: 215.4511
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.782569    0.837809   0.884060   0.943224   0.991655   1.066921   1.128377   1.227874   1.312521   1.480608  ...   2.915508   3.615216   4.198974   4.793491   5.634890   6.801401   7.854156   9.109937   9.356326   9.694568
r_loss     0.580498    0.600391   0.623037   0.645933   0.673863   0.703590   0.739119   0.772802   0.810680   0.854049  ...   1.111606   1.200189   1.268149   1.340454   1.416570   1.508423   1.592471   1.689624   1.743267   1.811700
d_loss   112.261623  101.028801  90.008144  79.277674  69.085131  60.555042  52.602449  47.403405  42.528941  40.818224  ...  45.959290  50.000544  50.794191  51.461062  52.630318  56.791613  56.234258  57.299712  49.195852  43.793709

[3 rows x 23 columns]
[training] In epoch 13, lr: 0.0001, Aux lr: 0.001
[Training] epoch  13: [   0/14786] | RD Loss: 1.9008 | R:0.9557, D:98.4445 | Aux loss: 213.91 | Batch time: 0.7639 |
[Training] epoch  13: [ 500/14786] | RD Loss: 2.7046 | R:1.1371, D:57.6665 | Aux loss: 221.53 | Batch time: 0.5794 |
[Training] epoch  13: [1000/14786] | RD Loss: 2.6829 | R:1.1372, D:58.4108 | Aux loss: 192.36 | Batch time: 0.5773 |
[Training] epoch  13: [1500/14786] | RD Loss: 2.6810 | R:1.1362, D:58.1960 | Aux loss: 226.96 | Batch time: 0.5783 |
[Training] epoch  13: [2000/14786] | RD Loss: 2.6714 | R:1.1338, D:58.6304 | Aux loss: 246.16 | Batch time: 0.5785 |
[Training] epoch  13: [2500/14786] | RD Loss: 2.6739 | R:1.1359, D:58.6600 | Aux loss: 202.93 | Batch time: 0.5777 |
[Training] epoch  13: [3000/14786] | RD Loss: 2.6702 | R:1.1342, D:58.5190 | Aux loss: 213.06 | Batch time: 0.5781 |
[Training] epoch  13: [3500/14786] | RD Loss: 2.6645 | R:1.1341, D:58.4298 | Aux loss: 223.91 | Batch time: 0.5784 |
[Training] epoch  13: [4000/14786] | RD Loss: 2.6772 | R:1.1351, D:58.6194 | Aux loss: 213.41 | Batch time: 0.5778 |
[Training] epoch  13: [4500/14786] | RD Loss: 2.6718 | R:1.1361, D:58.4863 | Aux loss: 228.39 | Batch time: 0.5781 |
[Training] epoch  13: [5000/14786] | RD Loss: 2.6742 | R:1.1345, D:58.6620 | Aux loss: 199.72 | Batch time: 0.5783 |
[Training] epoch  13: [5500/14786] | RD Loss: 2.6713 | R:1.1348, D:58.7298 | Aux loss: 223.83 | Batch time: 0.5779 |
[Training] epoch  13: [6000/14786] | RD Loss: 2.6656 | R:1.1353, D:58.6504 | Aux loss: 231.78 | Batch time: 0.5781 |
[Training] epoch  13: [6500/14786] | RD Loss: 2.6603 | R:1.1351, D:58.5039 | Aux loss: 207.65 | Batch time: 0.5782 |
[Training] epoch  13: [7000/14786] | RD Loss: 2.6601 | R:1.1349, D:58.5191 | Aux loss: 208.10 | Batch time: 0.5780 |
[Evaluating] epoch  13: RD Loss: 1.9198 | best RD loss: 1.6290 | R loss: 0.7770 | D Loss: 46.8935 | Aux loss: 200.6684
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.429938    0.494097   0.554913   0.629976   0.705479   0.804868   0.895424   1.026239   1.144216   1.279197  ...   1.723149   1.853262   2.010155   2.171298   2.486512   2.908334   3.526708   4.238904   4.907569   5.948331
r_loss     0.219669    0.243325   0.270199   0.299515   0.333775   0.373937   0.417583   0.464802   0.517044   0.573375  ...   0.857825   0.946860   1.011603   1.083213   1.155561   1.238689   1.320153   1.411262   1.469486   1.545214
d_loss   116.816179  106.711481  98.177255  88.122846  80.805383  71.821827  64.573092  58.483042  53.150197  45.981908  ...  22.046486  18.766092  17.305933  16.215867  16.605751  17.914644  19.816389  21.835075  22.217015  24.461757

[3 rows x 23 columns]
[Training] epoch  13: [7500/14786] | RD Loss: 2.6558 | R:1.1348, D:58.4561 | Aux loss: 218.97 | Batch time: 0.5781 |
[Training] epoch  13: [8000/14786] | RD Loss: 2.6511 | R:1.1341, D:58.2949 | Aux loss: 245.08 | Batch time: 0.5782 |
[Training] epoch  13: [8500/14786] | RD Loss: 2.6509 | R:1.1338, D:58.2981 | Aux loss: 204.88 | Batch time: 0.5780 |
[Training] epoch  13: [9000/14786] | RD Loss: 2.6527 | R:1.1333, D:58.3097 | Aux loss: 212.12 | Batch time: 0.5781 |
[Training] epoch  13: [9500/14786] | RD Loss: 2.6544 | R:1.1334, D:58.4221 | Aux loss: 229.21 | Batch time: 0.5782 |
[Training] epoch  13: [10000/14786] | RD Loss: 2.6542 | R:1.1340, D:58.3799 | Aux loss: 217.78 | Batch time: 0.5781 |
[Training] epoch  13: [10500/14786] | RD Loss: 2.6508 | R:1.1340, D:58.3327 | Aux loss: 206.43 | Batch time: 0.5781 |
[Training] epoch  13: [11000/14786] | RD Loss: 2.6490 | R:1.1339, D:58.3137 | Aux loss: 219.98 | Batch time: 0.5782 |
[Training] epoch  13: [11500/14786] | RD Loss: 2.6462 | R:1.1338, D:58.3021 | Aux loss: 208.78 | Batch time: 0.5781 |
[Training] epoch  13: [12000/14786] | RD Loss: 2.6449 | R:1.1337, D:58.2634 | Aux loss: 229.25 | Batch time: 0.5782 |
[Training] epoch  13: [12500/14786] | RD Loss: 2.6428 | R:1.1335, D:58.2525 | Aux loss: 200.50 | Batch time: 0.5782 |
[Training] epoch  13: [13000/14786] | RD Loss: 2.6406 | R:1.1334, D:58.2497 | Aux loss: 206.21 | Batch time: 0.5782 |
[Training] epoch  13: [13500/14786] | RD Loss: 2.6398 | R:1.1335, D:58.2138 | Aux loss: 246.65 | Batch time: 0.5781 |
[Training] epoch  13: [14000/14786] | RD Loss: 2.6415 | R:1.1338, D:58.2781 | Aux loss: 203.64 | Batch time: 0.5782 |
[Training] epoch  13: [14500/14786] | RD Loss: 2.6392 | R:1.1333, D:58.2752 | Aux loss: 209.38 | Batch time: 0.5782 |
[Evaluating] epoch  13: RD Loss: 3.1302 | best RD loss: 1.6290 | R loss: 1.0062 | D Loss: 55.7779 | Aux loss: 210.0543
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.673859   0.720646   0.759198   0.815099   0.870671   0.955890   1.036853   1.158987   1.279242   1.471405  ...   3.143388   4.035398   4.868623   5.874525   6.171947   6.427147   6.466367   6.289013   6.473102   6.290877
r_loss    0.498369   0.515528   0.533485   0.552929   0.580387   0.610185   0.644609   0.685124   0.725653   0.770237  ...   1.064157   1.167341   1.243854   1.329059   1.393745   1.472123   1.545012   1.631677   1.699140   1.784033
d_loss   97.494600  87.284458  77.832128  69.911908  63.105168  57.617502  53.005924  49.360682  46.914255  45.678737  ...  52.974021  59.380053  62.820951  67.741656  59.615754  53.165496  44.197170  35.963987  30.849514  25.038022

[3 rows x 23 columns]
[training] In epoch 14, lr: 0.0001, Aux lr: 0.001
[Training] epoch  14: [   0/14786] | RD Loss: 0.9946 | R:0.5763, D:90.9264 | Aux loss: 226.90 | Batch time: 0.7325 |
[Training] epoch  14: [ 500/14786] | RD Loss: 2.6135 | R:1.1305, D:57.7629 | Aux loss: 225.95 | Batch time: 0.5838 |
[Training] epoch  14: [1000/14786] | RD Loss: 2.5978 | R:1.1243, D:57.9957 | Aux loss: 203.65 | Batch time: 0.5812 |
[Training] epoch  14: [1500/14786] | RD Loss: 2.5923 | R:1.1202, D:58.1186 | Aux loss: 216.56 | Batch time: 0.5799 |
[Training] epoch  14: [2000/14786] | RD Loss: 2.6113 | R:1.1220, D:58.6390 | Aux loss: 231.92 | Batch time: 0.5800 |
[Training] epoch  14: [2500/14786] | RD Loss: 2.6040 | R:1.1222, D:58.3506 | Aux loss: 222.23 | Batch time: 0.5797 |
[Training] epoch  14: [3000/14786] | RD Loss: 2.6110 | R:1.1253, D:58.7377 | Aux loss: 212.46 | Batch time: 0.5792 |
[Training] epoch  14: [3500/14786] | RD Loss: 2.6086 | R:1.1248, D:58.7760 | Aux loss: 233.20 | Batch time: 0.5793 |
[Training] epoch  14: [4000/14786] | RD Loss: 2.6036 | R:1.1247, D:58.5989 | Aux loss: 220.62 | Batch time: 0.5793 |
[Training] epoch  14: [4500/14786] | RD Loss: 2.6007 | R:1.1245, D:58.5001 | Aux loss: 218.17 | Batch time: 0.5790 |
[Training] epoch  14: [5000/14786] | RD Loss: 2.5987 | R:1.1246, D:58.4810 | Aux loss: 210.05 | Batch time: 0.5791 |
[Training] epoch  14: [5500/14786] | RD Loss: 2.6006 | R:1.1265, D:58.4953 | Aux loss: 236.97 | Batch time: 0.5792 |
[Training] epoch  14: [6000/14786] | RD Loss: 2.6039 | R:1.1283, D:58.3818 | Aux loss: 226.07 | Batch time: 0.5789 |
[Training] epoch  14: [6500/14786] | RD Loss: 2.6028 | R:1.1279, D:58.3405 | Aux loss: 215.87 | Batch time: 0.5790 |
[Training] epoch  14: [7000/14786] | RD Loss: 2.6062 | R:1.1280, D:58.5049 | Aux loss: 215.64 | Batch time: 0.5791 |
[Evaluating] epoch  14: RD Loss: 4.0446 | best RD loss: 1.6290 | R loss: 2.2398 | D Loss: 51.7472 | Aux loss: 226.6814
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    2.126547    2.187056    2.241649   2.291459   2.335927   2.401132   2.448350   2.510256   2.558471   2.648972  ...   3.117603   3.408828   3.898934   4.533613   5.363777   6.446566   7.280338   8.077817   8.926090   9.771405
r_loss     1.906111    1.923959    1.943490   1.953832   1.974209   1.993497   2.013498   2.029150   2.044601   2.068427  ...   2.261394   2.329034   2.373731   2.429311   2.486929   2.551803   2.611249   2.671270   2.718612   2.786797
d_loss   122.464019  111.956301  102.813630  90.033861  78.634274  67.939106  58.763817  50.115195  43.548267  37.820506  ...  21.814249  22.355970  26.433331  31.360697  35.893301  41.789299  41.931644  41.749395  40.112945  38.803373

[3 rows x 23 columns]
[Training] epoch  14: [7500/14786] | RD Loss: 2.6074 | R:1.1288, D:58.4416 | Aux loss: 240.81 | Batch time: 0.5788 |
[Training] epoch  14: [8000/14786] | RD Loss: 2.6076 | R:1.1295, D:58.3891 | Aux loss: 209.00 | Batch time: 0.5789 |
[Training] epoch  14: [8500/14786] | RD Loss: 2.6069 | R:1.1296, D:58.4386 | Aux loss: 225.64 | Batch time: 0.5789 |
[Training] epoch  14: [9000/14786] | RD Loss: 2.6077 | R:1.1293, D:58.4781 | Aux loss: 229.55 | Batch time: 0.5788 |
[Training] epoch  14: [9500/14786] | RD Loss: 2.6082 | R:1.1297, D:58.4032 | Aux loss: 238.85 | Batch time: 0.5789 |
[Training] epoch  14: [10000/14786] | RD Loss: 2.6110 | R:1.1300, D:58.4166 | Aux loss: 220.70 | Batch time: 0.5789 |
[Training] epoch  14: [10500/14786] | RD Loss: 2.6084 | R:1.1297, D:58.4275 | Aux loss: 217.48 | Batch time: 0.5787 |
[Training] epoch  14: [11000/14786] | RD Loss: 2.6065 | R:1.1295, D:58.3622 | Aux loss: 234.68 | Batch time: 0.5788 |
[Training] epoch  14: [11500/14786] | RD Loss: 2.6025 | R:1.1290, D:58.2342 | Aux loss: 253.00 | Batch time: 0.5789 |
[Training] epoch  14: [12000/14786] | RD Loss: 2.6013 | R:1.1286, D:58.2063 | Aux loss: 216.57 | Batch time: 0.5787 |
[Training] epoch  14: [12500/14786] | RD Loss: 2.5989 | R:1.1287, D:58.1123 | Aux loss: 235.83 | Batch time: 0.5788 |
[Training] epoch  14: [13000/14786] | RD Loss: 2.6003 | R:1.1291, D:58.0870 | Aux loss: 232.55 | Batch time: 0.5788 |
[Training] epoch  14: [13500/14786] | RD Loss: 2.6014 | R:1.1295, D:58.0773 | Aux loss: 235.34 | Batch time: 0.5787 |
[Training] epoch  14: [14000/14786] | RD Loss: 2.5991 | R:1.1297, D:58.0305 | Aux loss: 221.99 | Batch time: 0.5787 |
[Training] epoch  14: [14500/14786] | RD Loss: 2.5960 | R:1.1291, D:57.9984 | Aux loss: 235.78 | Batch time: 0.5788 |
[Evaluating] epoch  14: RD Loss: 2.5235 | best RD loss: 1.6290 | R loss: 1.3290 | D Loss: 53.5666 | Aux loss: 214.1408
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    1.106755   1.156136   1.201637   1.282633   1.365049   1.481396   1.596808   1.751989   1.911319   2.169206  ...   3.352716   3.582446   3.607996   3.547989   3.441257   3.294539   3.239011   3.301423   3.562983   3.828138
r_loss     0.916589   0.931869   0.952865   0.976568   1.000853   1.026230   1.056331   1.084518   1.122357   1.150767  ...   1.374095   1.442535   1.494569   1.553249   1.609110   1.675745   1.746345   1.827499   1.895437   1.978250
d_loss   105.648018  95.432898  85.783491  81.617179  79.172964  75.860943  73.037448  69.528317  66.861173  66.347821  ...  50.410713  44.304568  36.627858  29.727859  22.858979  17.369029  13.405167  11.381652  10.775736  10.277156

[3 rows x 23 columns]
[training] In epoch 15, lr: 0.0001, Aux lr: 0.001
[Training] epoch  15: [   0/14786] | RD Loss: 0.8075 | R:0.4589, D:148.3761 | Aux loss: 223.37 | Batch time: 0.7800 |
[Training] epoch  15: [ 500/14786] | RD Loss: 2.6522 | R:1.1348, D:60.0296 | Aux loss: 221.91 | Batch time: 0.5780 |
[Training] epoch  15: [1000/14786] | RD Loss: 2.6444 | R:1.1314, D:59.6088 | Aux loss: 231.52 | Batch time: 0.5792 |
[Training] epoch  15: [1500/14786] | RD Loss: 2.6309 | R:1.1371, D:58.7795 | Aux loss: 254.56 | Batch time: 0.5786 |
[Training] epoch  15: [2000/14786] | RD Loss: 2.6398 | R:1.1416, D:58.6457 | Aux loss: 217.45 | Batch time: 0.5782 |
[Training] epoch  15: [2500/14786] | RD Loss: 2.6267 | R:1.1402, D:58.6097 | Aux loss: 231.36 | Batch time: 0.5787 |
[Training] epoch  15: [3000/14786] | RD Loss: 2.6298 | R:1.1402, D:58.6170 | Aux loss: 223.53 | Batch time: 0.5787 |
[Training] epoch  15: [3500/14786] | RD Loss: 2.6200 | R:1.1395, D:58.3435 | Aux loss: 243.10 | Batch time: 0.5785 |
[Training] epoch  15: [4000/14786] | RD Loss: 2.6102 | R:1.1388, D:58.1350 | Aux loss: 204.72 | Batch time: 0.5787 |
[Training] epoch  15: [4500/14786] | RD Loss: 2.6129 | R:1.1395, D:58.3483 | Aux loss: 228.37 | Batch time: 0.5787 |
[Training] epoch  15: [5000/14786] | RD Loss: 2.6069 | R:1.1377, D:58.1226 | Aux loss: 220.16 | Batch time: 0.5785 |
[Training] epoch  15: [5500/14786] | RD Loss: 2.6064 | R:1.1390, D:58.0202 | Aux loss: 247.03 | Batch time: 0.5787 |
[Training] epoch  15: [6000/14786] | RD Loss: 2.6012 | R:1.1378, D:57.9513 | Aux loss: 214.62 | Batch time: 0.5788 |
[Training] epoch  15: [6500/14786] | RD Loss: 2.5958 | R:1.1369, D:57.9207 | Aux loss: 244.56 | Batch time: 0.5785 |
[Training] epoch  15: [7000/14786] | RD Loss: 2.5930 | R:1.1373, D:57.9130 | Aux loss: 253.07 | Batch time: 0.5787 |
[Evaluating] epoch  15: RD Loss: 2.1490 | best RD loss: 1.6290 | R loss: 0.6803 | D Loss: 56.5318 | Aux loss: 234.3860
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.411718    0.491109    0.564609    0.631305   0.682346   0.763603   0.817126   0.878942   0.903180   1.003062  ...   1.711693   2.022123   2.418316   2.864457   3.280364   3.712326   4.238305   4.938864   5.955151   7.373937
r_loss     0.125650    0.146301    0.170129    0.196443   0.226783   0.263317   0.304941   0.348948   0.398392   0.454748  ...   0.745242   0.838364   0.910067   0.988445   1.067932   1.154644   1.246123   1.349200   1.426649   1.517721
d_loss   158.926401  146.726575  136.027614  115.963080  99.035472  83.381062  69.214245  55.207772  42.778681  35.720784  ...  24.622958  24.508468  26.139502  27.958443  27.603635  27.442936  26.871861  27.719414  29.263340  32.534531

[3 rows x 23 columns]
[Training] epoch  15: [7500/14786] | RD Loss: 2.5923 | R:1.1383, D:57.8045 | Aux loss: 249.02 | Batch time: 0.5787 |
[Training] epoch  15: [8000/14786] | RD Loss: 2.5897 | R:1.1379, D:57.6611 | Aux loss: 201.46 | Batch time: 0.5785 |
[Training] epoch  15: [8500/14786] | RD Loss: 2.5901 | R:1.1378, D:57.6228 | Aux loss: 230.94 | Batch time: 0.5786 |
[Training] epoch  15: [9000/14786] | RD Loss: 2.5894 | R:1.1376, D:57.6089 | Aux loss: 209.04 | Batch time: 0.5786 |
[Training] epoch  15: [9500/14786] | RD Loss: 2.5903 | R:1.1382, D:57.5451 | Aux loss: 210.75 | Batch time: 0.5785 |
[Training] epoch  15: [10000/14786] | RD Loss: 2.5916 | R:1.1379, D:57.5680 | Aux loss: 212.50 | Batch time: 0.5786 |
[Training] epoch  15: [10500/14786] | RD Loss: 2.5895 | R:1.1373, D:57.5560 | Aux loss: 246.13 | Batch time: 0.5786 |
[Training] epoch  15: [11000/14786] | RD Loss: 2.5891 | R:1.1375, D:57.5678 | Aux loss: 236.16 | Batch time: 0.5784 |
[Training] epoch  15: [11500/14786] | RD Loss: 2.5892 | R:1.1379, D:57.5118 | Aux loss: 216.71 | Batch time: 0.5785 |
[Training] epoch  15: [12000/14786] | RD Loss: 2.5882 | R:1.1377, D:57.5348 | Aux loss: 219.85 | Batch time: 0.5786 |
[Training] epoch  15: [12500/14786] | RD Loss: 2.5871 | R:1.1377, D:57.5218 | Aux loss: 235.06 | Batch time: 0.5784 |
[Training] epoch  15: [13000/14786] | RD Loss: 2.5875 | R:1.1373, D:57.5587 | Aux loss: 244.16 | Batch time: 0.5785 |
[Training] epoch  15: [13500/14786] | RD Loss: 2.5853 | R:1.1373, D:57.4793 | Aux loss: 202.65 | Batch time: 0.5785 |
[Training] epoch  15: [14000/14786] | RD Loss: 2.5848 | R:1.1373, D:57.4866 | Aux loss: 225.59 | Batch time: 0.5784 |
[Training] epoch  15: [14500/14786] | RD Loss: 2.5847 | R:1.1367, D:57.4943 | Aux loss: 237.61 | Batch time: 0.5785 |
[Evaluating] epoch  15: RD Loss: 2.3836 | best RD loss: 1.6290 | R loss: 1.3703 | D Loss: 46.9434 | Aux loss: 218.1294
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    1.181504    1.240401   1.294801   1.369004   1.434494   1.513258   1.570344   1.624975   1.678841   1.798947  ...   2.742917   3.005042   3.157631   3.266225   3.294476   3.283810   3.283302   3.386376   3.749686   4.330355
r_loss     0.979205    0.992854   1.008258   1.027586   1.049052   1.071035   1.095319   1.129828   1.169749   1.214199  ...   1.402644   1.470220   1.521648   1.578745   1.638896   1.706041   1.776625   1.856265   1.924035   1.996392
d_loss   112.387952  105.339245  98.807641  91.044725  83.791688  73.703971  64.192605  51.577811  43.143407  38.094309  ...  34.147067  31.776854  28.353257  25.148730  20.656013  16.928854  13.531004  11.815529  11.797420  12.966456

[3 rows x 23 columns]
[training] In epoch 16, lr: 0.0001, Aux lr: 0.001
[Training] epoch  16: [   0/14786] | RD Loss: 6.6959 | R:2.3488, D:28.0912 | Aux loss: 217.67 | Batch time: 0.7473 |
[Training] epoch  16: [ 500/14786] | RD Loss: 2.5644 | R:1.1219, D:59.0284 | Aux loss: 211.63 | Batch time: 0.5758 |
[Training] epoch  16: [1000/14786] | RD Loss: 2.5716 | R:1.1279, D:58.5904 | Aux loss: 228.42 | Batch time: 0.5768 |
[Training] epoch  16: [1500/14786] | RD Loss: 2.5804 | R:1.1295, D:58.5501 | Aux loss: 233.11 | Batch time: 0.5782 |
[Training] epoch  16: [2000/14786] | RD Loss: 2.5878 | R:1.1331, D:58.1530 | Aux loss: 210.62 | Batch time: 0.5795 |
[Training] epoch  16: [2500/14786] | RD Loss: 2.5880 | R:1.1311, D:58.5735 | Aux loss: 239.58 | Batch time: 0.5797 |
[Training] epoch  16: [3000/14786] | RD Loss: 2.5847 | R:1.1323, D:58.4689 | Aux loss: 276.26 | Batch time: 0.5800 |
[Training] epoch  16: [3500/14786] | RD Loss: 2.5833 | R:1.1332, D:58.3060 | Aux loss: 229.82 | Batch time: 0.5797 |
[Training] epoch  16: [4000/14786] | RD Loss: 2.5893 | R:1.1331, D:58.2394 | Aux loss: 205.60 | Batch time: 0.5795 |
[Training] epoch  16: [4500/14786] | RD Loss: 2.5862 | R:1.1327, D:58.0998 | Aux loss: 216.18 | Batch time: 0.5794 |
[Training] epoch  16: [5000/14786] | RD Loss: 2.5855 | R:1.1335, D:57.9998 | Aux loss: 235.90 | Batch time: 0.5795 |
[Training] epoch  16: [5500/14786] | RD Loss: 2.5920 | R:1.1362, D:58.1292 | Aux loss: 219.23 | Batch time: 0.5795 |
[Training] epoch  16: [6000/14786] | RD Loss: 2.5911 | R:1.1370, D:58.2023 | Aux loss: 223.96 | Batch time: 0.5797 |
[Training] epoch  16: [6500/14786] | RD Loss: 2.5897 | R:1.1360, D:58.2163 | Aux loss: 223.79 | Batch time: 0.5797 |
[Training] epoch  16: [7000/14786] | RD Loss: 2.5898 | R:1.1363, D:58.0550 | Aux loss: 240.14 | Batch time: 0.5795 |
[Evaluating] epoch  16: RD Loss: 1.8724 | best RD loss: 1.6290 | R loss: 0.6964 | D Loss: 41.2548 | Aux loss: 238.1657
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.321533   0.374082   0.423627   0.477983   0.525174   0.591314   0.652537   0.745468   0.829808   0.955714  ...   1.680314   1.968009   2.264918   2.616253   2.960416   3.376551   3.862284   4.401069   4.849780   5.445330
r_loss     0.130937   0.150820   0.173721   0.199863   0.229813   0.265499   0.305733   0.349950   0.399636   0.454971  ...   0.747616   0.842009   0.918766   1.003592   1.095662   1.198958   1.297472   1.409887   1.491374   1.585255
d_loss   105.887117  95.005167  86.174318  74.165369  64.208962  54.302376  46.865520  41.199849  36.455239  32.621735  ...  23.763011  23.312646  23.330197  24.033698  23.265800  23.364731  23.033784  23.097932  21.702138  21.444863

[3 rows x 23 columns]
[Training] epoch  16: [7500/14786] | RD Loss: 2.5918 | R:1.1361, D:58.0292 | Aux loss: 219.47 | Batch time: 0.5796 |
[Training] epoch  16: [8000/14786] | RD Loss: 2.5892 | R:1.1356, D:57.9610 | Aux loss: 236.84 | Batch time: 0.5800 |
[Training] epoch  16: [8500/14786] | RD Loss: 2.5842 | R:1.1347, D:57.9489 | Aux loss: 233.38 | Batch time: 0.5806 |
[Training] epoch  16: [9000/14786] | RD Loss: 2.5817 | R:1.1352, D:57.8467 | Aux loss: 247.40 | Batch time: 0.5812 |
[Training] epoch  16: [9500/14786] | RD Loss: 2.5858 | R:1.1370, D:57.8870 | Aux loss: 214.86 | Batch time: 0.5820 |
[Training] epoch  16: [10000/14786] | RD Loss: 2.5883 | R:1.1370, D:57.9316 | Aux loss: 225.67 | Batch time: 0.5831 |
[Training] epoch  16: [10500/14786] | RD Loss: 2.5897 | R:1.1368, D:58.0371 | Aux loss: 222.32 | Batch time: 0.5840 |
[Training] epoch  16: [11000/14786] | RD Loss: 2.5896 | R:1.1370, D:58.0602 | Aux loss: 263.46 | Batch time: 0.5845 |
[Training] epoch  16: [11500/14786] | RD Loss: 2.5902 | R:1.1375, D:58.0152 | Aux loss: 226.91 | Batch time: 0.5850 |
[Training] epoch  16: [12000/14786] | RD Loss: 2.5900 | R:1.1372, D:58.0506 | Aux loss: 219.13 | Batch time: 0.5853 |
[Training] epoch  16: [12500/14786] | RD Loss: 2.5924 | R:1.1379, D:58.0629 | Aux loss: 222.30 | Batch time: 0.5857 |
[Training] epoch  16: [13000/14786] | RD Loss: 2.5933 | R:1.1382, D:58.0552 | Aux loss: 216.40 | Batch time: 0.5861 |
[Training] epoch  16: [13500/14786] | RD Loss: 2.5896 | R:1.1378, D:58.0112 | Aux loss: 214.70 | Batch time: 0.5862 |
[Training] epoch  16: [14000/14786] | RD Loss: 2.5878 | R:1.1374, D:57.9875 | Aux loss: 223.19 | Batch time: 0.5859 |
[Training] epoch  16: [14500/14786] | RD Loss: 2.5865 | R:1.1369, D:57.9831 | Aux loss: 210.11 | Batch time: 0.5856 |
[Evaluating] epoch  16: RD Loss: 1.7854 | best RD loss: 1.6290 | R loss: 0.7009 | D Loss: 63.8713 | Aux loss: 225.8666
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600     0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.399966    0.496496    0.587208    0.725426    0.847648    0.996813    1.131421   1.284513   1.408703   1.521032  ...   1.867419   1.897945   1.949622   2.027577   2.212458   2.401133   2.704717   3.113435   3.813250   4.681209
r_loss     0.142288    0.163340    0.186977    0.214079    0.244917    0.280841    0.321449   0.367091   0.418040   0.474255  ...   0.768005   0.860232   0.931000   1.009010   1.089461   1.178402   1.269984   1.372301   1.453697   1.546742
d_loss   143.154209  141.768771  138.010783  136.359305  131.028466  119.328696  109.455749  95.564833  83.954525  68.193921  ...  28.010549  21.484736  17.653760  15.179836  14.011193  13.119436  12.884894  13.445048  15.247513  17.413710

[3 rows x 23 columns]
[training] In epoch 17, lr: 0.0001, Aux lr: 0.001
[Training] epoch  17: [   0/14786] | RD Loss: 3.8802 | R:1.7207, D:23.1708 | Aux loss: 225.74 | Batch time: 0.7972 |
[Training] epoch  17: [ 500/14786] | RD Loss: 2.5563 | R:1.1348, D:58.8728 | Aux loss: 218.13 | Batch time: 0.5935 |
[Training] epoch  17: [1000/14786] | RD Loss: 2.5525 | R:1.1349, D:57.7546 | Aux loss: 231.27 | Batch time: 0.5940 |
[Training] epoch  17: [1500/14786] | RD Loss: 2.5457 | R:1.1361, D:57.3884 | Aux loss: 210.41 | Batch time: 0.5949 |
[Training] epoch  17: [2000/14786] | RD Loss: 2.5399 | R:1.1328, D:57.1160 | Aux loss: 238.65 | Batch time: 0.5951 |
[Training] epoch  17: [2500/14786] | RD Loss: 2.5447 | R:1.1312, D:57.2718 | Aux loss: 245.54 | Batch time: 0.5994 |
[Training] epoch  17: [3000/14786] | RD Loss: 2.5509 | R:1.1316, D:57.2923 | Aux loss: 239.30 | Batch time: 0.6038 |
[Training] epoch  17: [3500/14786] | RD Loss: 2.5617 | R:1.1344, D:57.2529 | Aux loss: 202.73 | Batch time: 0.6058 |
[Training] epoch  17: [4000/14786] | RD Loss: 2.5609 | R:1.1341, D:57.4966 | Aux loss: 236.89 | Batch time: 0.6070 |
[Training] epoch  17: [4500/14786] | RD Loss: 2.5640 | R:1.1364, D:57.4996 | Aux loss: 233.83 | Batch time: 0.6096 |
[Training] epoch  17: [5000/14786] | RD Loss: 2.5648 | R:1.1365, D:57.4896 | Aux loss: 223.08 | Batch time: 0.6106 |
[Training] epoch  17: [5500/14786] | RD Loss: 2.5625 | R:1.1357, D:57.3543 | Aux loss: 210.04 | Batch time: 0.6118 |
[Training] epoch  17: [6000/14786] | RD Loss: 2.5558 | R:1.1341, D:57.3735 | Aux loss: 221.76 | Batch time: 0.6138 |
[Training] epoch  17: [6500/14786] | RD Loss: 2.5552 | R:1.1350, D:57.3325 | Aux loss: 237.46 | Batch time: 0.6148 |
[Training] epoch  17: [7000/14786] | RD Loss: 2.5591 | R:1.1355, D:57.4445 | Aux loss: 232.69 | Batch time: 0.6152 |
[Evaluating] epoch  17: RD Loss: 2.1655 | best RD loss: 1.6290 | R loss: 0.7579 | D Loss: 44.0183 | Aux loss: 241.5450
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600   0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770   0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.408820   0.454629   0.494397   0.550265   0.598887   0.672658   0.74012   0.836642   0.936022   1.089898  ...   2.176004   2.586487   2.967832   3.37863   3.812156   4.344505   4.584680   4.753930   4.841300   5.006262
r_loss    0.235265   0.252755   0.272986   0.296240   0.323198   0.355144   0.39164   0.431408   0.476489   0.528538  ...   0.802377   0.893698   0.967794   1.04939   1.134444   1.229150   1.324019   1.429646   1.500277   1.580457
d_loss   96.419137  85.903725  76.348619  67.740075  59.932437  52.918966  47.09195  42.211797  38.943476  36.570699  ...  34.996868  35.047389  34.662708  34.71296  33.408760  33.426561  29.282986  25.670153  21.589813  19.032249

[3 rows x 23 columns]
[Training] epoch  17: [7500/14786] | RD Loss: 2.5577 | R:1.1353, D:57.3699 | Aux loss: 220.67 | Batch time: 0.6163 |
[Training] epoch  17: [8000/14786] | RD Loss: 2.5537 | R:1.1353, D:57.3274 | Aux loss: 232.07 | Batch time: 0.6172 |
[Training] epoch  17: [8500/14786] | RD Loss: 2.5546 | R:1.1356, D:57.3445 | Aux loss: 238.12 | Batch time: 0.6181 |
[Training] epoch  17: [9000/14786] | RD Loss: 2.5572 | R:1.1361, D:57.3534 | Aux loss: 213.92 | Batch time: 0.6190 |
[Training] epoch  17: [9500/14786] | RD Loss: 2.5544 | R:1.1353, D:57.3004 | Aux loss: 216.56 | Batch time: 0.6205 |
[Training] epoch  17: [10000/14786] | RD Loss: 2.5535 | R:1.1354, D:57.3095 | Aux loss: 214.82 | Batch time: 0.6218 |
[Training] epoch  17: [10500/14786] | RD Loss: 2.5533 | R:1.1352, D:57.3139 | Aux loss: 249.58 | Batch time: 0.6226 |
[Training] epoch  17: [11000/14786] | RD Loss: 2.5529 | R:1.1355, D:57.2819 | Aux loss: 241.71 | Batch time: 0.6239 |
[Training] epoch  17: [11500/14786] | RD Loss: 2.5504 | R:1.1354, D:57.2529 | Aux loss: 229.86 | Batch time: 0.6235 |
[Training] epoch  17: [12000/14786] | RD Loss: 2.5485 | R:1.1348, D:57.2916 | Aux loss: 233.81 | Batch time: 0.6240 |
[Training] epoch  17: [12500/14786] | RD Loss: 2.5483 | R:1.1348, D:57.3540 | Aux loss: 262.02 | Batch time: 0.6241 |
[Training] epoch  17: [13000/14786] | RD Loss: 2.5474 | R:1.1347, D:57.3162 | Aux loss: 215.22 | Batch time: 0.6229 |
[Training] epoch  17: [13500/14786] | RD Loss: 2.5461 | R:1.1340, D:57.3068 | Aux loss: 216.77 | Batch time: 0.6219 |
[Training] epoch  17: [14000/14786] | RD Loss: 2.5458 | R:1.1341, D:57.3058 | Aux loss: 255.14 | Batch time: 0.6222 |
[Training] epoch  17: [14500/14786] | RD Loss: 2.5471 | R:1.1344, D:57.3973 | Aux loss: 235.11 | Batch time: 0.6213 |
[Evaluating] epoch  17: RD Loss: 2.0024 | best RD loss: 1.6290 | R loss: 0.8968 | D Loss: 44.2468 | Aux loss: 214.2776
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.616034    0.680911    0.735272   0.792873   0.836026   0.891487   0.939235   1.017226   1.094038   1.203940  ...   1.740481   1.902089   2.078130   2.308595   2.652719   3.064991   3.599806   4.279914   5.137238   6.152246
r_loss     0.393423    0.412509    0.434686   0.457488   0.483542   0.516268   0.551604   0.588870   0.632275   0.681032  ...   0.942406   1.028608   1.096158   1.172333   1.249612   1.333909   1.423281   1.523937   1.606486   1.703241
d_loss   123.672845  114.213984  103.650609  89.436142  76.626880  62.536591  52.382620  44.620427  39.132462  34.065624  ...  20.333120  18.084493  17.018589  16.933850  17.506010  18.573842  19.546701  21.281676  22.815847  24.716692

[3 rows x 23 columns]
[training] In epoch 18, lr: 0.0001, Aux lr: 0.001
[Training] epoch  18: [   0/14786] | RD Loss: 4.1560 | R:1.9376, D:38.4471 | Aux loss: 212.21 | Batch time: 0.6351 |
[Training] epoch  18: [ 500/14786] | RD Loss: 2.5760 | R:1.1353, D:58.5244 | Aux loss: 221.84 | Batch time: 0.5946 |
[Training] epoch  18: [1000/14786] | RD Loss: 2.5624 | R:1.1378, D:58.1587 | Aux loss: 242.17 | Batch time: 0.5962 |
[Training] epoch  18: [1500/14786] | RD Loss: 2.5583 | R:1.1356, D:57.6564 | Aux loss: 220.69 | Batch time: 0.5958 |
[Training] epoch  18: [2000/14786] | RD Loss: 2.5630 | R:1.1370, D:57.8034 | Aux loss: 221.21 | Batch time: 0.6054 |
[Training] epoch  18: [2500/14786] | RD Loss: 2.5650 | R:1.1363, D:57.8453 | Aux loss: 215.91 | Batch time: 0.6098 |
[Training] epoch  18: [3000/14786] | RD Loss: 2.5615 | R:1.1333, D:57.7574 | Aux loss: 223.23 | Batch time: 0.6082 |
[Training] epoch  18: [3500/14786] | RD Loss: 2.5723 | R:1.1325, D:58.1263 | Aux loss: 226.07 | Batch time: 0.6085 |
[Training] epoch  18: [4000/14786] | RD Loss: 2.5731 | R:1.1321, D:58.6156 | Aux loss: 234.36 | Batch time: 0.6067 |
[Training] epoch  18: [4500/14786] | RD Loss: 2.5834 | R:1.1316, D:58.7203 | Aux loss: 239.82 | Batch time: 0.6040 |
[Training] epoch  18: [5000/14786] | RD Loss: 2.5864 | R:1.1313, D:58.7541 | Aux loss: 256.99 | Batch time: 0.6019 |
[Training] epoch  18: [5500/14786] | RD Loss: 2.5806 | R:1.1299, D:58.6289 | Aux loss: 223.51 | Batch time: 0.6001 |
[Training] epoch  18: [6000/14786] | RD Loss: 2.5777 | R:1.1290, D:58.5855 | Aux loss: 232.40 | Batch time: 0.5986 |
[Training] epoch  18: [6500/14786] | RD Loss: 2.5775 | R:1.1288, D:58.5667 | Aux loss: 241.04 | Batch time: 0.5971 |
[Training] epoch  18: [7000/14786] | RD Loss: 2.5788 | R:1.1295, D:58.4654 | Aux loss: 221.51 | Batch time: 0.5959 |
[Evaluating] epoch  18: RD Loss: 1.6476 | best RD loss: 1.6290 | R loss: 0.6873 | D Loss: 40.5105 | Aux loss: 229.6799
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.307233   0.358499   0.404672   0.470817   0.535157   0.620551   0.701413   0.812992   0.926761   1.083776  ...   1.961395   2.238910   2.392591   2.527257   2.602885   2.695337   2.856829   3.076226   3.340054   3.617287
r_loss    0.139184   0.159876   0.183926   0.210432   0.240621   0.276198   0.315909   0.358687   0.406661   0.460996  ...   0.741711   0.832053   0.905052   0.984208   1.066849   1.155853   1.253268   1.360670   1.446027   1.539377
d_loss   93.360936  84.520729  76.119371  69.435869  64.029412  57.392175  52.095040  47.323448  44.076255  40.571969  ...  31.074735  29.127461  25.780569  22.996265  19.164515  16.518069  14.401088  13.247537  12.239272  11.543947

[3 rows x 23 columns]
[Training] epoch  18: [7500/14786] | RD Loss: 2.5721 | R:1.1286, D:58.2444 | Aux loss: 227.05 | Batch time: 0.5948 |
[Training] epoch  18: [8000/14786] | RD Loss: 2.5686 | R:1.1287, D:58.1725 | Aux loss: 239.44 | Batch time: 0.5942 |
[Training] epoch  18: [8500/14786] | RD Loss: 2.5705 | R:1.1300, D:58.1108 | Aux loss: 251.54 | Batch time: 0.5940 |
[Training] epoch  18: [9000/14786] | RD Loss: 2.5673 | R:1.1305, D:57.9448 | Aux loss: 217.39 | Batch time: 0.5935 |
[Training] epoch  18: [9500/14786] | RD Loss: 2.5635 | R:1.1303, D:57.8610 | Aux loss: 222.96 | Batch time: 0.5927 |
[Training] epoch  18: [10000/14786] | RD Loss: 2.5606 | R:1.1301, D:57.8377 | Aux loss: 231.00 | Batch time: 0.5922 |
[Training] epoch  18: [10500/14786] | RD Loss: 2.5635 | R:1.1304, D:57.7992 | Aux loss: 248.26 | Batch time: 0.5919 |
[Training] epoch  18: [11000/14786] | RD Loss: 2.5611 | R:1.1305, D:57.6512 | Aux loss: 202.70 | Batch time: 0.5915 |
[Training] epoch  18: [11500/14786] | RD Loss: 2.5569 | R:1.1305, D:57.5232 | Aux loss: 229.17 | Batch time: 0.5909 |
[Training] epoch  18: [12000/14786] | RD Loss: 2.5541 | R:1.1304, D:57.4032 | Aux loss: 262.72 | Batch time: 0.5905 |
[Training] epoch  18: [12500/14786] | RD Loss: 2.5523 | R:1.1309, D:57.3530 | Aux loss: 244.02 | Batch time: 0.5901 |
[Training] epoch  18: [13000/14786] | RD Loss: 2.5497 | R:1.1312, D:57.2288 | Aux loss: 237.03 | Batch time: 0.5901 |
[Training] epoch  18: [13500/14786] | RD Loss: 2.5466 | R:1.1310, D:57.1407 | Aux loss: 253.67 | Batch time: 0.5898 |
[Training] epoch  18: [14000/14786] | RD Loss: 2.5440 | R:1.1307, D:57.1120 | Aux loss: 243.15 | Batch time: 0.5895 |
[Training] epoch  18: [14500/14786] | RD Loss: 2.5432 | R:1.1311, D:57.0660 | Aux loss: 238.06 | Batch time: 0.5893 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch  18: RD Loss: 1.6259 | best RD loss: 1.6259 | R loss: 0.6810 | D Loss: 41.1556 | Aux loss: 257.5180
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.324863   0.382446   0.437465   0.503801   0.564128   0.644839   0.716432   0.807607   0.882007   0.978934  ...   1.493160   1.710698   1.955492   2.252441   2.607457   3.002207   3.285987   3.547206   3.746001   4.021970
r_loss     0.128493   0.149158   0.173261   0.199595   0.230092   0.265478   0.305583   0.349686   0.399015   0.452990  ...   0.739120   0.833187   0.907550   0.988003   1.069829   1.156980   1.251363   1.353340   1.434502   1.525989
d_loss   109.094552  99.271754  91.104777  81.121569  72.616464  63.226976  55.520136  47.700103  40.931524  34.263461  ...  19.211203  18.167924  18.161904  18.844077  19.184379  19.798573  18.272331  16.941048  14.936989  13.866559

[3 rows x 23 columns]
[training] In epoch 19, lr: 0.0001, Aux lr: 0.001
[Training] epoch  19: [   0/14786] | RD Loss: 1.8116 | R:1.0340, D:25.7506 | Aux loss: 252.34 | Batch time: 0.7528 |
[Training] epoch  19: [ 500/14786] | RD Loss: 2.4861 | R:1.1406, D:54.6977 | Aux loss: 247.43 | Batch time: 0.5902 |
[Training] epoch  19: [1000/14786] | RD Loss: 2.5165 | R:1.1426, D:54.6236 | Aux loss: 238.26 | Batch time: 0.5927 |
[Training] epoch  19: [1500/14786] | RD Loss: 2.4973 | R:1.1382, D:54.6108 | Aux loss: 230.04 | Batch time: 0.5916 |
[Training] epoch  19: [2000/14786] | RD Loss: 2.4866 | R:1.1348, D:54.5682 | Aux loss: 241.86 | Batch time: 0.5904 |
[Training] epoch  19: [2500/14786] | RD Loss: 2.4976 | R:1.1346, D:54.7384 | Aux loss: 255.92 | Batch time: 0.5888 |
[Training] epoch  19: [3000/14786] | RD Loss: 2.5031 | R:1.1339, D:54.7617 | Aux loss: 227.82 | Batch time: 0.5882 |
[Training] epoch  19: [3500/14786] | RD Loss: 2.5092 | R:1.1355, D:55.0374 | Aux loss: 248.96 | Batch time: 0.5900 |
[Training] epoch  19: [4000/14786] | RD Loss: 2.5142 | R:1.1358, D:55.2756 | Aux loss: 249.59 | Batch time: 0.5921 |
[Training] epoch  19: [4500/14786] | RD Loss: 2.5277 | R:1.1375, D:55.5487 | Aux loss: 252.76 | Batch time: 0.5935 |
[Training] epoch  19: [5000/14786] | RD Loss: 2.5297 | R:1.1377, D:55.5101 | Aux loss: 227.20 | Batch time: 0.5950 |
[Training] epoch  19: [5500/14786] | RD Loss: 2.5251 | R:1.1374, D:55.6248 | Aux loss: 236.58 | Batch time: 0.5960 |
[Training] epoch  19: [6000/14786] | RD Loss: 2.5259 | R:1.1369, D:55.7886 | Aux loss: 245.44 | Batch time: 0.5975 |
[Training] epoch  19: [6500/14786] | RD Loss: 2.5271 | R:1.1375, D:55.8034 | Aux loss: 247.14 | Batch time: 0.5978 |
[Training] epoch  19: [7000/14786] | RD Loss: 2.5269 | R:1.1380, D:55.7362 | Aux loss: 232.20 | Batch time: 0.5975 |
[Evaluating] epoch  19: RD Loss: 2.5333 | best RD loss: 1.6259 | R loss: 0.7037 | D Loss: 65.0176 | Aux loss: 216.4897
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.322436   0.380256   0.433656   0.518858   0.614942   0.762410   0.929995   1.155367   1.396793   1.714495  ...   3.297121   3.755154   4.060375   4.356240   4.518822   4.654717   4.690276   4.450580   4.502976   4.449604
r_loss     0.132505   0.153286   0.177589   0.203594   0.233898   0.270139   0.311033   0.356309   0.406714   0.463499  ...   0.761967   0.859010   0.937011   1.021057   1.106216   1.198535   1.297202   1.405354   1.495284   1.596174
d_loss   105.517608  96.583010  88.299091  84.070567  82.835551  82.045224  83.643542  83.235293  83.904969  81.498132  ...  64.589890  59.961574  54.131091  49.704654  42.577741  37.083495  30.472156  23.515256  19.435808  15.852389

[3 rows x 23 columns]
[Training] epoch  19: [7500/14786] | RD Loss: 2.5299 | R:1.1380, D:55.8544 | Aux loss: 246.07 | Batch time: 0.5964 |
[Training] epoch  19: [8000/14786] | RD Loss: 2.5281 | R:1.1377, D:55.8667 | Aux loss: 250.22 | Batch time: 0.5955 |
[Training] epoch  19: [8500/14786] | RD Loss: 2.5292 | R:1.1376, D:55.8127 | Aux loss: 244.50 | Batch time: 0.5949 |
[Training] epoch  19: [9000/14786] | RD Loss: 2.5292 | R:1.1374, D:55.8431 | Aux loss: 217.93 | Batch time: 0.5942 |
[Training] epoch  19: [9500/14786] | RD Loss: 2.5283 | R:1.1368, D:55.8559 | Aux loss: 237.47 | Batch time: 0.5935 |
[Training] epoch  19: [10000/14786] | RD Loss: 2.5264 | R:1.1361, D:55.8937 | Aux loss: 252.37 | Batch time: 0.5930 |
[Training] epoch  19: [10500/14786] | RD Loss: 2.5276 | R:1.1366, D:55.8878 | Aux loss: 235.18 | Batch time: 0.5926 |
[Training] epoch  19: [11000/14786] | RD Loss: 2.5265 | R:1.1363, D:55.8871 | Aux loss: 226.40 | Batch time: 0.5920 |
[Training] epoch  19: [11500/14786] | RD Loss: 2.5249 | R:1.1360, D:55.9104 | Aux loss: 234.99 | Batch time: 0.5917 |
[Training] epoch  19: [12000/14786] | RD Loss: 2.5261 | R:1.1364, D:55.9857 | Aux loss: 235.27 | Batch time: 0.5913 |
[Training] epoch  19: [12500/14786] | RD Loss: 2.5340 | R:1.1371, D:56.0744 | Aux loss: 219.00 | Batch time: 0.5909 |
[Training] epoch  19: [13000/14786] | RD Loss: 2.5335 | R:1.1373, D:56.0254 | Aux loss: 270.37 | Batch time: 0.5907 |
[Training] epoch  19: [13500/14786] | RD Loss: 2.5337 | R:1.1373, D:56.0570 | Aux loss: 241.45 | Batch time: 0.5904 |
[Training] epoch  19: [14000/14786] | RD Loss: 2.5333 | R:1.1370, D:56.0505 | Aux loss: 253.80 | Batch time: 0.5900 |
[Training] epoch  19: [14500/14786] | RD Loss: 2.5337 | R:1.1373, D:55.9862 | Aux loss: 229.69 | Batch time: 0.5897 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch  19: RD Loss: 1.5780 | best RD loss: 1.5780 | R loss: 0.7056 | D Loss: 35.2285 | Aux loss: 251.0971
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.300733   0.349883   0.395426   0.452072   0.501816   0.574519   0.638534   0.727880   0.808670   0.927329  ...   1.545919   1.767114   2.002796   2.268239   2.534491   2.802069   3.089547   3.376518   3.676898   3.995875
r_loss    0.134109   0.155173   0.179569   0.206142   0.236808   0.273447   0.315113   0.360542   0.411178   0.468936  ...   0.765539   0.861008   0.939268   1.024631   1.110135   1.202205   1.297363   1.399991   1.487479   1.582740
d_loss   92.568891  82.855445  74.433217  65.581233  57.610608  50.178761  43.705481  38.264327  33.685766  29.862779  ...  19.882277  18.759963  18.432025  18.533649  17.771129  17.165921  16.095050  15.262761  14.148107  13.406302

[3 rows x 23 columns]
[training] In epoch 20, lr: 0.0001, Aux lr: 0.001
[Training] epoch  20: [   0/14786] | RD Loss: 1.5577 | R:0.8573, D:45.6308 | Aux loss: 245.87 | Batch time: 0.7956 |
[Training] epoch  20: [ 500/14786] | RD Loss: 2.5312 | R:1.1407, D:56.0133 | Aux loss: 267.10 | Batch time: 0.5994 |
[Training] epoch  20: [1000/14786] | RD Loss: 2.5607 | R:1.1428, D:56.9846 | Aux loss: 219.70 | Batch time: 0.6147 |
[Training] epoch  20: [1500/14786] | RD Loss: 2.5469 | R:1.1331, D:57.3785 | Aux loss: 229.75 | Batch time: 0.6176 |
[Training] epoch  20: [2000/14786] | RD Loss: 2.5531 | R:1.1325, D:57.4981 | Aux loss: 225.11 | Batch time: 0.6197 |
[Training] epoch  20: [2500/14786] | RD Loss: 2.5599 | R:1.1388, D:57.2872 | Aux loss: 237.47 | Batch time: 0.6234 |
[Training] epoch  20: [3000/14786] | RD Loss: 2.5491 | R:1.1379, D:56.9714 | Aux loss: 233.32 | Batch time: 0.6224 |
[Training] epoch  20: [3500/14786] | RD Loss: 2.5418 | R:1.1387, D:56.9414 | Aux loss: 242.34 | Batch time: 0.6188 |
[Training] epoch  20: [4000/14786] | RD Loss: 2.5367 | R:1.1373, D:56.8098 | Aux loss: 271.25 | Batch time: 0.6145 |
[Training] epoch  20: [4500/14786] | RD Loss: 2.5390 | R:1.1390, D:56.7732 | Aux loss: 231.44 | Batch time: 0.6125 |
[Training] epoch  20: [5000/14786] | RD Loss: 2.5355 | R:1.1373, D:56.7605 | Aux loss: 228.97 | Batch time: 0.6123 |
[Training] epoch  20: [5500/14786] | RD Loss: 2.5341 | R:1.1373, D:56.7390 | Aux loss: 251.42 | Batch time: 0.6102 |
[Training] epoch  20: [6000/14786] | RD Loss: 2.5341 | R:1.1379, D:56.5348 | Aux loss: 245.51 | Batch time: 0.6090 |
[Training] epoch  20: [6500/14786] | RD Loss: 2.5322 | R:1.1385, D:56.3962 | Aux loss: 224.67 | Batch time: 0.6070 |
[Training] epoch  20: [7000/14786] | RD Loss: 2.5330 | R:1.1390, D:56.4751 | Aux loss: 224.81 | Batch time: 0.6055 |
[Evaluating] epoch  20: RD Loss: 1.6765 | best RD loss: 1.5780 | R loss: 0.6955 | D Loss: 42.4642 | Aux loss: 237.1023
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.329196   0.383951   0.434395   0.500888   0.563482   0.655044   0.741122   0.855382   0.960897   1.099111  ...   1.670778   1.806154   1.919936   2.032986   2.210527   2.430547   2.886030   3.478688   4.226396   5.267501
r_loss     0.137214   0.156651   0.179575   0.204748   0.234297   0.268641   0.307550   0.351389   0.400036   0.455311  ...   0.745733   0.839126   0.914463   0.997620   1.085126   1.181853   1.280996   1.391810   1.484480   1.615038
d_loss   106.656664  96.723240  87.869179  78.970861  71.562032  64.400410  58.590779  52.499199  47.530522  41.941362  ...  23.568029  20.021276  17.425870  15.430187  14.041183  13.398003  14.414317  16.114889  17.718355  20.291462

[3 rows x 23 columns]
[Training] epoch  20: [7500/14786] | RD Loss: 2.5299 | R:1.1387, D:56.3480 | Aux loss: 263.09 | Batch time: 0.6041 |
[Training] epoch  20: [8000/14786] | RD Loss: 2.5320 | R:1.1390, D:56.3114 | Aux loss: 232.31 | Batch time: 0.6030 |
[Training] epoch  20: [8500/14786] | RD Loss: 2.5305 | R:1.1399, D:56.1846 | Aux loss: 216.05 | Batch time: 0.6022 |
[Training] epoch  20: [9000/14786] | RD Loss: 2.5274 | R:1.1393, D:56.1312 | Aux loss: 226.71 | Batch time: 0.6019 |
[Training] epoch  20: [9500/14786] | RD Loss: 2.5264 | R:1.1388, D:56.0975 | Aux loss: 265.35 | Batch time: 0.6014 |
[Training] epoch  20: [10000/14786] | RD Loss: 2.5265 | R:1.1391, D:56.0627 | Aux loss: 244.64 | Batch time: 0.6016 |
[Training] epoch  20: [10500/14786] | RD Loss: 2.5267 | R:1.1389, D:56.0668 | Aux loss: 251.52 | Batch time: 0.6017 |
[Training] epoch  20: [11000/14786] | RD Loss: 2.5248 | R:1.1388, D:56.0371 | Aux loss: 238.05 | Batch time: 0.6017 |
[Training] epoch  20: [11500/14786] | RD Loss: 2.5246 | R:1.1394, D:56.0094 | Aux loss: 240.09 | Batch time: 0.6020 |
[Training] epoch  20: [12000/14786] | RD Loss: 2.5227 | R:1.1397, D:55.9204 | Aux loss: 249.87 | Batch time: 0.6022 |
[Training] epoch  20: [12500/14786] | RD Loss: 2.5227 | R:1.1397, D:55.8758 | Aux loss: 231.25 | Batch time: 0.6017 |
[Training] epoch  20: [13000/14786] | RD Loss: 2.5229 | R:1.1394, D:55.9207 | Aux loss: 242.77 | Batch time: 0.6013 |
[Training] epoch  20: [13500/14786] | RD Loss: 2.5244 | R:1.1394, D:55.9998 | Aux loss: 252.52 | Batch time: 0.6009 |
[Training] epoch  20: [14000/14786] | RD Loss: 2.5227 | R:1.1395, D:55.9211 | Aux loss: 258.17 | Batch time: 0.6005 |
[Training] epoch  20: [14500/14786] | RD Loss: 2.5231 | R:1.1394, D:55.9867 | Aux loss: 236.19 | Batch time: 0.6002 |
[Evaluating] epoch  20: RD Loss: 2.0355 | best RD loss: 1.5780 | R loss: 0.6909 | D Loss: 41.6094 | Aux loss: 260.3453
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.304881   0.352165   0.394307   0.451546   0.501803   0.577926   0.648986   0.749108   0.847941   0.992373  ...   1.977020   2.350661   2.678142   3.029818   3.496878   3.988931   4.301485   4.577694   5.027956   5.412953
r_loss    0.137203   0.157854   0.181409   0.208004   0.238716   0.273900   0.314178   0.357636   0.406714   0.462160  ...   0.751216   0.842932   0.916365   0.996080   1.080195   1.170984   1.264728   1.365839   1.445559   1.533841
d_loss   93.154333  82.685733  73.413043  64.944514  57.192803  50.670921  45.244353  40.778305  37.392119  34.541518  ...  31.230679  31.215930  30.533393  30.309063  30.152012  30.235480  27.272174  24.801973  23.149574  21.550619

[3 rows x 23 columns]
[training] In epoch 21, lr: 0.0001, Aux lr: 0.001
[Training] epoch  21: [   0/14786] | RD Loss: 1.2527 | R:0.7215, D:71.7742 | Aux loss: 256.68 | Batch time: 0.7508 |
[Training] epoch  21: [ 500/14786] | RD Loss: 2.4473 | R:1.1359, D:54.6529 | Aux loss: 245.32 | Batch time: 0.5897 |
[Training] epoch  21: [1000/14786] | RD Loss: 2.4362 | R:1.1425, D:54.3702 | Aux loss: 237.15 | Batch time: 0.5911 |
[Training] epoch  21: [1500/14786] | RD Loss: 2.4295 | R:1.1388, D:54.2580 | Aux loss: 256.10 | Batch time: 0.5900 |
[Training] epoch  21: [2000/14786] | RD Loss: 2.4476 | R:1.1403, D:54.5492 | Aux loss: 232.50 | Batch time: 0.5903 |
[Training] epoch  21: [2500/14786] | RD Loss: 2.4677 | R:1.1431, D:54.9462 | Aux loss: 233.65 | Batch time: 0.5965 |
[Training] epoch  21: [3000/14786] | RD Loss: 2.4574 | R:1.1411, D:54.8328 | Aux loss: 276.19 | Batch time: 0.6036 |
[Training] epoch  21: [3500/14786] | RD Loss: 2.4600 | R:1.1391, D:55.0125 | Aux loss: 241.21 | Batch time: 0.6069 |
[Training] epoch  21: [4000/14786] | RD Loss: 2.4632 | R:1.1405, D:54.9071 | Aux loss: 264.63 | Batch time: 0.6085 |
[Training] epoch  21: [4500/14786] | RD Loss: 2.4680 | R:1.1419, D:54.9006 | Aux loss: 220.30 | Batch time: 0.6072 |
[Training] epoch  21: [5000/14786] | RD Loss: 2.4673 | R:1.1417, D:54.8673 | Aux loss: 241.30 | Batch time: 0.6051 |
[Training] epoch  21: [5500/14786] | RD Loss: 2.4663 | R:1.1411, D:54.8387 | Aux loss: 263.06 | Batch time: 0.6032 |
[Training] epoch  21: [6000/14786] | RD Loss: 2.4645 | R:1.1417, D:54.7869 | Aux loss: 258.65 | Batch time: 0.6021 |
[Training] epoch  21: [6500/14786] | RD Loss: 2.4627 | R:1.1419, D:54.7660 | Aux loss: 227.28 | Batch time: 0.6009 |
[Training] epoch  21: [7000/14786] | RD Loss: 2.4584 | R:1.1408, D:54.7178 | Aux loss: 247.69 | Batch time: 0.6001 |
[Evaluating] update best loss and save best model...
[Evaluating] epoch  21: RD Loss: 1.3977 | best RD loss: 1.3977 | R loss: 0.6925 | D Loss: 35.0171 | Aux loss: 249.4967
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.299420   0.351712   0.400079   0.464707   0.520520   0.595172   0.660908   0.751105   0.831460   0.941054  ...   1.437265   1.586682   1.725619   1.868017   2.046049   2.228773   2.489724   2.788871   3.149209   3.537721
r_loss    0.127022   0.147661   0.171967   0.198469   0.229125   0.264825   0.305358   0.350227   0.400253   0.457522  ...   0.753447   0.847402   0.922687   1.004267   1.088921   1.180429   1.277668   1.384913   1.471127   1.563834
d_loss   95.776828  86.830054  78.659393  70.996978  63.346696  55.057901  48.047280  41.758087  36.542938  31.500473  ...  17.422111  15.305986  13.915618  12.872581  11.941709  11.248327  10.885099  10.841374  10.843829  10.966041

[3 rows x 23 columns]
[Training] epoch  21: [7500/14786] | RD Loss: 2.4611 | R:1.1417, D:54.6797 | Aux loss: 299.75 | Batch time: 0.5997 |
[Training] epoch  21: [8000/14786] | RD Loss: 2.4599 | R:1.1421, D:54.6147 | Aux loss: 248.54 | Batch time: 0.5990 |
[Training] epoch  21: [8500/14786] | RD Loss: 2.4577 | R:1.1416, D:54.6354 | Aux loss: 235.04 | Batch time: 0.5980 |
[Training] epoch  21: [9000/14786] | RD Loss: 2.4597 | R:1.1412, D:54.7370 | Aux loss: 262.07 | Batch time: 0.5975 |
[Training] epoch  21: [9500/14786] | RD Loss: 2.4577 | R:1.1408, D:54.6884 | Aux loss: 261.37 | Batch time: 0.5967 |
[Training] epoch  21: [10000/14786] | RD Loss: 2.4613 | R:1.1418, D:54.6975 | Aux loss: 247.19 | Batch time: 0.5967 |
[Training] epoch  21: [10500/14786] | RD Loss: 2.4611 | R:1.1421, D:54.6809 | Aux loss: 239.14 | Batch time: 0.5961 |
[Training] epoch  21: [11000/14786] | RD Loss: 2.4610 | R:1.1419, D:54.6989 | Aux loss: 268.65 | Batch time: 0.5954 |
[Training] epoch  21: [11500/14786] | RD Loss: 2.4626 | R:1.1419, D:54.7178 | Aux loss: 251.44 | Batch time: 0.5949 |
[Training] epoch  21: [12000/14786] | RD Loss: 2.4618 | R:1.1417, D:54.6466 | Aux loss: 230.69 | Batch time: 0.5943 |
[Training] epoch  21: [12500/14786] | RD Loss: 2.4595 | R:1.1412, D:54.5857 | Aux loss: 235.61 | Batch time: 0.5940 |
[Training] epoch  21: [13000/14786] | RD Loss: 2.4563 | R:1.1407, D:54.5176 | Aux loss: 257.28 | Batch time: 0.5933 |
[Training] epoch  21: [13500/14786] | RD Loss: 2.4550 | R:1.1413, D:54.4757 | Aux loss: 277.67 | Batch time: 0.5928 |
[Training] epoch  21: [14000/14786] | RD Loss: 2.4552 | R:1.1413, D:54.4534 | Aux loss: 235.10 | Batch time: 0.5922 |
[Training] epoch  21: [14500/14786] | RD Loss: 2.4537 | R:1.1412, D:54.3908 | Aux loss: 239.33 | Batch time: 0.5918 |
[Evaluating] epoch  21: RD Loss: 1.5612 | best RD loss: 1.3977 | R loss: 0.6981 | D Loss: 38.8048 | Aux loss: 229.5861
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950   0.15475   0.18000
rd_loss   0.290482   0.337993   0.380261   0.441998   0.500608   0.588795   0.675996   0.796301   0.917602   1.077053  ...   1.910614   2.144589   2.270487   2.378941   2.470021   2.578638   2.694305   2.819825  3.032033  3.249614
r_loss    0.123710   0.143713   0.167087   0.193215   0.223081   0.259420   0.299834   0.345418   0.396029   0.454518  ...   0.759136   0.857032   0.934447   1.019060   1.107983   1.204759   1.302085   1.409102  1.492442  1.581111
d_loss   92.651565  82.672210  73.508200  66.341922  60.331951  54.895902  50.832656  46.966888  44.201138  40.556024  ...  29.337003  26.657491  23.154927  20.266473  16.993601  14.741193  12.503099  10.893615  9.948892  9.269463

[3 rows x 23 columns]
[training] In epoch 22, lr: 0.0001, Aux lr: 0.001
[Training] epoch  22: [   0/14786] | RD Loss: 0.5484 | R:0.3579, D:50.7917 | Aux loss: 242.09 | Batch time: 0.7572 |
[Training] epoch  22: [ 500/14786] | RD Loss: 2.4775 | R:1.1556, D:55.2728 | Aux loss: 239.97 | Batch time: 0.5752 |
[Training] epoch  22: [1000/14786] | RD Loss: 2.4466 | R:1.1440, D:55.4180 | Aux loss: 238.48 | Batch time: 0.5780 |
[Training] epoch  22: [1500/14786] | RD Loss: 2.4427 | R:1.1465, D:54.9620 | Aux loss: 259.65 | Batch time: 0.5789 |
[Training] epoch  22: [2000/14786] | RD Loss: 2.4420 | R:1.1459, D:54.2927 | Aux loss: 253.32 | Batch time: 0.5782 |
[Training] epoch  22: [2500/14786] | RD Loss: 2.4387 | R:1.1429, D:54.2824 | Aux loss: 236.27 | Batch time: 0.5786 |
[Training] epoch  22: [3000/14786] | RD Loss: 2.4477 | R:1.1450, D:54.5715 | Aux loss: 253.60 | Batch time: 0.5790 |
[Training] epoch  22: [3500/14786] | RD Loss: 2.4532 | R:1.1465, D:54.6355 | Aux loss: 280.49 | Batch time: 0.5788 |
[Training] epoch  22: [4000/14786] | RD Loss: 2.4556 | R:1.1469, D:54.5266 | Aux loss: 256.77 | Batch time: 0.5789 |
[Training] epoch  22: [4500/14786] | RD Loss: 2.4550 | R:1.1464, D:54.6482 | Aux loss: 262.52 | Batch time: 0.5792 |
[Training] epoch  22: [5000/14786] | RD Loss: 2.4556 | R:1.1460, D:54.7477 | Aux loss: 235.20 | Batch time: 0.5790 |
[Training] epoch  22: [5500/14786] | RD Loss: 2.4611 | R:1.1455, D:54.8324 | Aux loss: 278.67 | Batch time: 0.5792 |
[Training] epoch  22: [6000/14786] | RD Loss: 2.4643 | R:1.1472, D:54.8594 | Aux loss: 234.40 | Batch time: 0.5793 |
[Training] epoch  22: [6500/14786] | RD Loss: 2.4621 | R:1.1459, D:54.8356 | Aux loss: 238.25 | Batch time: 0.5792 |
[Training] epoch  22: [7000/14786] | RD Loss: 2.4654 | R:1.1461, D:54.8454 | Aux loss: 254.07 | Batch time: 0.5791 |
[Evaluating] epoch  22: RD Loss: 1.7822 | best RD loss: 1.3977 | R loss: 0.6942 | D Loss: 41.5603 | Aux loss: 247.1906
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.301598   0.355004   0.403997   0.475776   0.553738   0.654970   0.757200   0.868066   0.963854   1.052033  ...   1.469835   1.704954   1.975405   2.311331   2.740332   3.255983   3.645830   4.198885   4.619247   5.081531
r_loss    0.125529   0.145995   0.169685   0.196072   0.226854   0.262401   0.303029   0.348557   0.399523   0.457701  ...   0.758336   0.853489   0.926762   1.006460   1.092124   1.185019   1.282223   1.389727   1.477839   1.570088
d_loss   97.816424  88.940099  80.797222  74.587591  71.061576  65.428188  61.374418  54.115470  47.824658  38.718723  ...  18.127339  17.628681  18.174060  19.446663  20.564043  22.220644  21.226828  21.692339  20.299894  19.508018

[3 rows x 23 columns]
[Training] epoch  22: [7500/14786] | RD Loss: 2.4672 | R:1.1462, D:54.8797 | Aux loss: 257.19 | Batch time: 0.5793 |
[Training] epoch  22: [8000/14786] | RD Loss: 2.4639 | R:1.1464, D:54.7100 | Aux loss: 248.67 | Batch time: 0.5791 |
[Training] epoch  22: [8500/14786] | RD Loss: 2.4618 | R:1.1454, D:54.7092 | Aux loss: 240.60 | Batch time: 0.5791 |
[Training] epoch  22: [9000/14786] | RD Loss: 2.4630 | R:1.1448, D:54.8322 | Aux loss: 274.06 | Batch time: 0.5793 |
[Training] epoch  22: [9500/14786] | RD Loss: 2.4660 | R:1.1451, D:54.9093 | Aux loss: 239.75 | Batch time: 0.5791 |
[Training] epoch  22: [10000/14786] | RD Loss: 2.4655 | R:1.1454, D:54.8354 | Aux loss: 239.30 | Batch time: 0.5792 |
[Training] epoch  22: [10500/14786] | RD Loss: 2.4621 | R:1.1443, D:54.7986 | Aux loss: 243.02 | Batch time: 0.5793 |
[Training] epoch  22: [11000/14786] | RD Loss: 2.4646 | R:1.1447, D:54.8237 | Aux loss: 262.08 | Batch time: 0.5792 |
[Training] epoch  22: [11500/14786] | RD Loss: 2.4657 | R:1.1452, D:54.8366 | Aux loss: 279.83 | Batch time: 0.5792 |
[Training] epoch  22: [12000/14786] | RD Loss: 2.4641 | R:1.1447, D:54.7568 | Aux loss: 242.68 | Batch time: 0.5793 |
[Training] epoch  22: [12500/14786] | RD Loss: 2.4635 | R:1.1443, D:54.7483 | Aux loss: 282.89 | Batch time: 0.5792 |
[Training] epoch  22: [13000/14786] | RD Loss: 2.4646 | R:1.1445, D:54.7518 | Aux loss: 282.13 | Batch time: 0.5792 |
[Training] epoch  22: [13500/14786] | RD Loss: 2.4662 | R:1.1444, D:54.8153 | Aux loss: 224.12 | Batch time: 0.5793 |
[Training] epoch  22: [14000/14786] | RD Loss: 2.4647 | R:1.1442, D:54.7794 | Aux loss: 245.61 | Batch time: 0.5793 |
[Training] epoch  22: [14500/14786] | RD Loss: 2.4634 | R:1.1441, D:54.7567 | Aux loss: 240.96 | Batch time: 0.5793 |
[Evaluating] epoch  22: RD Loss: 1.6095 | best RD loss: 1.3977 | R loss: 0.6892 | D Loss: 46.5437 | Aux loss: 226.6789
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475   0.18000
rd_loss    0.323155   0.378684   0.429497   0.506082   0.585486   0.695768   0.807355   0.949651   1.098730   1.263730  ...   1.972943   2.111926   2.205453   2.279736   2.389949   2.482696   2.605179   2.765587   3.023723  3.312066
r_loss     0.127134   0.147157   0.170829   0.197105   0.227327   0.262648   0.303131   0.347764   0.397982   0.454633  ...   0.751296   0.844839   0.918802   0.999309   1.082337   1.172027   1.269149   1.374328   1.466528  1.563247
d_loss   108.900615  98.522053  89.195562  82.393889  77.860822  72.186742  68.138345  62.696573  59.385473  52.709875  ...  31.124763  26.233667  22.298971  19.082379  16.314567  14.062977  11.998476  10.743314  10.062650  9.715662

[3 rows x 23 columns]
[training] In epoch 23, lr: 0.0001, Aux lr: 0.001
[Training] epoch  23: [   0/14786] | RD Loss: 0.7721 | R:0.4770, D:163.9346 | Aux loss: 230.23 | Batch time: 0.7726 |
[Training] epoch  23: [ 500/14786] | RD Loss: 2.4141 | R:1.1303, D:54.0588 | Aux loss: 243.42 | Batch time: 0.5803 |
[Training] epoch  23: [1000/14786] | RD Loss: 2.4280 | R:1.1315, D:54.3037 | Aux loss: 281.32 | Batch time: 0.5789 |
[Training] epoch  23: [1500/14786] | RD Loss: 2.4185 | R:1.1348, D:53.5513 | Aux loss: 271.51 | Batch time: 0.5796 |
[Training] epoch  23: [2000/14786] | RD Loss: 2.4397 | R:1.1420, D:53.9613 | Aux loss: 234.63 | Batch time: 0.5800 |
[Training] epoch  23: [2500/14786] | RD Loss: 2.4239 | R:1.1399, D:53.7594 | Aux loss: 250.13 | Batch time: 0.5792 |
[Training] epoch  23: [3000/14786] | RD Loss: 2.4327 | R:1.1412, D:54.1088 | Aux loss: 259.01 | Batch time: 0.5796 |
[Training] epoch  23: [3500/14786] | RD Loss: 2.4334 | R:1.1426, D:53.9111 | Aux loss: 251.47 | Batch time: 0.5798 |
[Training] epoch  23: [4000/14786] | RD Loss: 2.4300 | R:1.1407, D:53.6983 | Aux loss: 237.51 | Batch time: 0.5792 |
[Training] epoch  23: [4500/14786] | RD Loss: 2.4208 | R:1.1391, D:53.6320 | Aux loss: 239.85 | Batch time: 0.5794 |
[Training] epoch  23: [5000/14786] | RD Loss: 2.4234 | R:1.1385, D:53.7421 | Aux loss: 264.88 | Batch time: 0.5796 |
[Training] epoch  23: [5500/14786] | RD Loss: 2.4278 | R:1.1403, D:53.7317 | Aux loss: 254.03 | Batch time: 0.5797 |
[Training] epoch  23: [6000/14786] | RD Loss: 2.4313 | R:1.1422, D:53.8109 | Aux loss: 233.76 | Batch time: 0.5798 |
[Training] epoch  23: [6500/14786] | RD Loss: 2.4329 | R:1.1421, D:53.8769 | Aux loss: 235.14 | Batch time: 0.5798 |
[Training] epoch  23: [7000/14786] | RD Loss: 2.4287 | R:1.1421, D:53.7561 | Aux loss: 279.52 | Batch time: 0.5797 |
[Evaluating] epoch  23: RD Loss: 1.9639 | best RD loss: 1.3977 | R loss: 0.7349 | D Loss: 52.7448 | Aux loss: 257.9528
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.426313    0.507352    0.587187    0.683078    0.766435   0.848727   0.887813   0.928690   0.940772   1.020396  ...   1.788665   2.066742   2.221339   2.401349   2.661420   2.998723   3.552800   4.330870   5.256219   6.364904
r_loss     0.179071    0.199971    0.224032    0.250110    0.280064   0.314386   0.354330   0.398136   0.446666   0.501541  ...   0.794613   0.887257   0.961619   1.042218   1.126225   1.216631   1.310741   1.412233   1.499749   1.594650
d_loss   137.356542  130.800509  125.226087  115.458053  105.732698  89.056725  72.092377  55.266022  41.873392  33.801649  ...  25.326174  24.419967  21.832228  20.255311  19.154028  19.121164  20.135237  22.537736  24.274440  26.501407

[3 rows x 23 columns]
[Training] epoch  23: [7500/14786] | RD Loss: 2.4332 | R:1.1430, D:53.7499 | Aux loss: 232.57 | Batch time: 0.5797 |
[Training] epoch  23: [8000/14786] | RD Loss: 2.4339 | R:1.1427, D:53.8424 | Aux loss: 236.18 | Batch time: 0.5798 |
[Training] epoch  23: [8500/14786] | RD Loss: 2.4349 | R:1.1424, D:53.9352 | Aux loss: 256.52 | Batch time: 0.5796 |
[Training] epoch  23: [9000/14786] | RD Loss: 2.4348 | R:1.1419, D:53.8973 | Aux loss: 283.11 | Batch time: 0.5797 |
[Training] epoch  23: [9500/14786] | RD Loss: 2.4403 | R:1.1431, D:53.9467 | Aux loss: 244.05 | Batch time: 0.5797 |
[Training] epoch  23: [10000/14786] | RD Loss: 2.4398 | R:1.1422, D:53.9227 | Aux loss: 238.53 | Batch time: 0.5795 |
[Training] epoch  23: [10500/14786] | RD Loss: 2.4398 | R:1.1414, D:53.9489 | Aux loss: 250.10 | Batch time: 0.5796 |
[Training] epoch  23: [11000/14786] | RD Loss: 2.4417 | R:1.1416, D:53.9836 | Aux loss: 254.00 | Batch time: 0.5796 |
[Training] epoch  23: [11500/14786] | RD Loss: 2.4419 | R:1.1416, D:53.9603 | Aux loss: 229.62 | Batch time: 0.5795 |
[Training] epoch  23: [12000/14786] | RD Loss: 2.4417 | R:1.1416, D:53.9366 | Aux loss: 243.24 | Batch time: 0.5795 |
[Training] epoch  23: [12500/14786] | RD Loss: 2.4425 | R:1.1419, D:53.9646 | Aux loss: 249.11 | Batch time: 0.5795 |
[Training] epoch  23: [13000/14786] | RD Loss: 2.4465 | R:1.1427, D:54.0677 | Aux loss: 258.90 | Batch time: 0.5794 |
[Training] epoch  23: [13500/14786] | RD Loss: 2.4495 | R:1.1431, D:54.1221 | Aux loss: 229.85 | Batch time: 0.5794 |
[Training] epoch  23: [14000/14786] | RD Loss: 2.4499 | R:1.1430, D:54.1489 | Aux loss: 238.05 | Batch time: 0.5795 |
[Training] epoch  23: [14500/14786] | RD Loss: 2.4500 | R:1.1431, D:54.1584 | Aux loss: 245.32 | Batch time: 0.5794 |
[Evaluating] epoch  23: RD Loss: 1.6222 | best RD loss: 1.3977 | R loss: 0.6787 | D Loss: 50.2545 | Aux loss: 233.0703
[Evaluating] detailed RD performance:
            0.00180     0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.317456    0.379221   0.440781   0.528982   0.631215   0.765839   0.911518   1.069692   1.224975   1.386479  ...   1.843309   1.925185   1.979698   2.050076   2.170131   2.305629   2.545341   2.846543   3.287848   3.793197
r_loss     0.120700    0.140326   0.163203   0.188738   0.218448   0.253968   0.294468   0.339292   0.389752   0.445070  ...   0.737495   0.832538   0.907098   0.988306   1.072011   1.162572   1.259386   1.363919   1.450617   1.545549
d_loss   109.309092  101.657310  95.716602  90.731761  89.732054  85.311872  83.385200  76.083268  70.781570  61.329586  ...  28.173602  22.622101  18.589252  15.823698  13.700815  12.264559  11.548759  11.448829  11.872251  12.486933

[3 rows x 23 columns]
[training] In epoch 24, lr: 0.0001, Aux lr: 0.001
[Training] epoch  24: [   0/14786] | RD Loss: 4.4552 | R:1.6674, D:21.5271 | Aux loss: 232.30 | Batch time: 0.7319 |
[Training] epoch  24: [ 500/14786] | RD Loss: 2.4072 | R:1.1393, D:54.8532 | Aux loss: 265.68 | Batch time: 0.5804 |
[Training] epoch  24: [1000/14786] | RD Loss: 2.4134 | R:1.1297, D:54.3904 | Aux loss: 269.81 | Batch time: 0.5796 |
[Training] epoch  24: [1500/14786] | RD Loss: 2.4456 | R:1.1343, D:54.3244 | Aux loss: 254.32 | Batch time: 0.5788 |
[Training] epoch  24: [2000/14786] | RD Loss: 2.4609 | R:1.1358, D:54.3956 | Aux loss: 239.66 | Batch time: 0.5792 |
[Training] epoch  24: [2500/14786] | RD Loss: 2.4727 | R:1.1388, D:54.8101 | Aux loss: 241.02 | Batch time: 0.5792 |
[Training] epoch  24: [3000/14786] | RD Loss: 2.4650 | R:1.1391, D:54.6944 | Aux loss: 254.51 | Batch time: 0.5787 |
[Training] epoch  24: [3500/14786] | RD Loss: 2.4588 | R:1.1393, D:54.3666 | Aux loss: 263.81 | Batch time: 0.5790 |
[Training] epoch  24: [4000/14786] | RD Loss: 2.4508 | R:1.1379, D:54.2949 | Aux loss: 242.68 | Batch time: 0.5791 |
[Training] epoch  24: [4500/14786] | RD Loss: 2.4538 | R:1.1388, D:54.3379 | Aux loss: 266.50 | Batch time: 0.5788 |
[Training] epoch  24: [5000/14786] | RD Loss: 2.4446 | R:1.1389, D:54.1144 | Aux loss: 287.09 | Batch time: 0.5789 |
[Training] epoch  24: [5500/14786] | RD Loss: 2.4501 | R:1.1410, D:54.0460 | Aux loss: 240.46 | Batch time: 0.5791 |
[Training] epoch  24: [6000/14786] | RD Loss: 2.4520 | R:1.1403, D:54.1577 | Aux loss: 248.80 | Batch time: 0.5788 |
[Training] epoch  24: [6500/14786] | RD Loss: 2.4511 | R:1.1390, D:54.2195 | Aux loss: 260.09 | Batch time: 0.5789 |
[Training] epoch  24: [7000/14786] | RD Loss: 2.4498 | R:1.1403, D:54.1216 | Aux loss: 264.45 | Batch time: 0.5791 |
[Evaluating] epoch  24: RD Loss: 3.0513 | best RD loss: 1.3977 | R loss: 0.6810 | D Loss: 59.7432 | Aux loss: 264.4312
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.308127   0.355050   0.395970   0.453550   0.503452   0.582721   0.655105   0.774297   0.910660   1.142768  ...   3.466693   4.398747   5.071430   5.680250   6.212755   6.682775   6.564724   6.489392   6.685753   6.823968
r_loss     0.122431   0.142238   0.165510   0.190839   0.220102   0.254362   0.292856   0.336755   0.386245   0.442519  ...   0.738740   0.834041   0.908015   0.988586   1.074038   1.166649   1.266141   1.374982   1.463292   1.560323
d_loss   103.164369  90.558292  79.468918  70.056361  61.597932  54.726630  48.952595  45.577258  44.441956  45.618870  ...  69.501990  73.803433  72.156236  69.920482  64.113750  59.185901  47.584946  39.493512  33.747729  29.242471

[3 rows x 23 columns]
[Training] epoch  24: [7500/14786] | RD Loss: 2.4505 | R:1.1411, D:54.0433 | Aux loss: 232.81 | Batch time: 0.5789 |
[Training] epoch  24: [8000/14786] | RD Loss: 2.4473 | R:1.1401, D:54.0523 | Aux loss: 245.15 | Batch time: 0.5790 |
[Training] epoch  24: [8500/14786] | RD Loss: 2.4501 | R:1.1410, D:54.1337 | Aux loss: 262.44 | Batch time: 0.5791 |
[Training] epoch  24: [9000/14786] | RD Loss: 2.4528 | R:1.1414, D:54.1835 | Aux loss: 269.76 | Batch time: 0.5789 |
[Training] epoch  24: [9500/14786] | RD Loss: 2.4510 | R:1.1412, D:54.1434 | Aux loss: 247.18 | Batch time: 0.5790 |
[Training] epoch  24: [10000/14786] | RD Loss: 2.4503 | R:1.1415, D:54.1558 | Aux loss: 244.17 | Batch time: 0.5791 |
[Training] epoch  24: [10500/14786] | RD Loss: 2.4513 | R:1.1411, D:54.1327 | Aux loss: 277.92 | Batch time: 0.5789 |
[Training] epoch  24: [11000/14786] | RD Loss: 2.4519 | R:1.1414, D:54.1369 | Aux loss: 268.21 | Batch time: 0.5790 |
[Training] epoch  24: [11500/14786] | RD Loss: 2.4479 | R:1.1415, D:54.0192 | Aux loss: 244.49 | Batch time: 0.5791 |
[Training] epoch  24: [12000/14786] | RD Loss: 2.4444 | R:1.1416, D:53.9439 | Aux loss: 253.90 | Batch time: 0.5789 |
[Training] epoch  24: [12500/14786] | RD Loss: 2.4407 | R:1.1416, D:53.8513 | Aux loss: 263.52 | Batch time: 0.5790 |
[Training] epoch  24: [13000/14786] | RD Loss: 2.4414 | R:1.1419, D:53.8997 | Aux loss: 258.45 | Batch time: 0.5790 |
[Training] epoch  24: [13500/14786] | RD Loss: 2.4395 | R:1.1418, D:53.8866 | Aux loss: 244.85 | Batch time: 0.5789 |
[Training] epoch  24: [14000/14786] | RD Loss: 2.4389 | R:1.1418, D:53.8954 | Aux loss: 225.78 | Batch time: 0.5789 |
[Training] epoch  24: [14500/14786] | RD Loss: 2.4376 | R:1.1419, D:53.8255 | Aux loss: 261.51 | Batch time: 0.5790 |
[Evaluating] epoch  24: RD Loss: 2.2660 | best RD loss: 1.3977 | R loss: 0.9635 | D Loss: 58.5671 | Aux loss: 250.3367
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290     0.00375     0.00460     0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.660304    0.736570    0.809678    0.915861    1.031236    1.172189   1.317311   1.478030   1.622434   1.764412  ...   2.047424   2.059137   2.127593   2.227204   2.469522   2.793019   3.420615   4.321045   5.698786   7.628429
r_loss     0.431833    0.453031    0.477297    0.503327    0.532926    0.568078   0.607495   0.645622   0.691557   0.744519  ...   1.020981   1.109438   1.176927   1.250601   1.329084   1.413765   1.504672   1.604369   1.689557   1.786208
d_loss   126.928748  120.654769  114.614281  110.009140  108.328385  100.685080  95.921059  86.709086  78.887863  66.442541  ...  26.151409  19.662509  16.476005  14.554443  14.228796  14.798864  17.206496  20.978194  25.907780  32.456781

[3 rows x 23 columns]
[training] In epoch 25, lr: 0.0001, Aux lr: 0.001
[Training] epoch  25: [   0/14786] | RD Loss: 3.8015 | R:1.7523, D:25.5664 | Aux loss: 247.07 | Batch time: 0.7868 |
[Training] epoch  25: [ 500/14786] | RD Loss: 2.5140 | R:1.1528, D:56.4827 | Aux loss: 281.09 | Batch time: 0.5798 |
[Training] epoch  25: [1000/14786] | RD Loss: 2.5014 | R:1.1592, D:55.7593 | Aux loss: 251.17 | Batch time: 0.5804 |
[Training] epoch  25: [1500/14786] | RD Loss: 2.4823 | R:1.1563, D:54.6057 | Aux loss: 255.70 | Batch time: 0.5799 |
[Training] epoch  25: [2000/14786] | RD Loss: 2.4773 | R:1.1533, D:54.8535 | Aux loss: 251.51 | Batch time: 0.5796 |
[Training] epoch  25: [2500/14786] | RD Loss: 2.4732 | R:1.1482, D:54.7608 | Aux loss: 277.18 | Batch time: 0.5799 |
[Training] epoch  25: [3000/14786] | RD Loss: 2.4781 | R:1.1474, D:55.0998 | Aux loss: 272.98 | Batch time: 0.5799 |
[Training] epoch  25: [3500/14786] | RD Loss: 2.4763 | R:1.1477, D:54.8501 | Aux loss: 259.13 | Batch time: 0.5796 |
[Training] epoch  25: [4000/14786] | RD Loss: 2.4687 | R:1.1471, D:54.8010 | Aux loss: 248.73 | Batch time: 0.5799 |
[Training] epoch  25: [4500/14786] | RD Loss: 2.4633 | R:1.1468, D:54.7224 | Aux loss: 282.56 | Batch time: 0.5798 |
[Training] epoch  25: [5000/14786] | RD Loss: 2.4549 | R:1.1455, D:54.4992 | Aux loss: 254.78 | Batch time: 0.5796 |
[Training] epoch  25: [5500/14786] | RD Loss: 2.4523 | R:1.1460, D:54.4303 | Aux loss: 244.19 | Batch time: 0.5798 |
[Training] epoch  25: [6000/14786] | RD Loss: 2.4544 | R:1.1457, D:54.4713 | Aux loss: 262.68 | Batch time: 0.5800 |
[Training] epoch  25: [6500/14786] | RD Loss: 2.4547 | R:1.1454, D:54.5336 | Aux loss: 254.74 | Batch time: 0.5808 |
[Training] epoch  25: [7000/14786] | RD Loss: 2.4544 | R:1.1458, D:54.4370 | Aux loss: 249.63 | Batch time: 0.5814 |
[Evaluating] epoch  25: RD Loss: 1.9297 | best RD loss: 1.3977 | R loss: 0.6897 | D Loss: 43.9219 | Aux loss: 253.5386
[Evaluating] detailed RD performance:
           0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss   0.292110   0.340729   0.383611   0.443898   0.499029   0.582685   0.665490   0.793729   0.935291   1.148357  ...   2.333414   2.672599   2.862819   3.043003   3.192616   3.343862   3.587278   3.847041   4.085148   4.290728
r_loss    0.124502   0.145395   0.170385   0.196618   0.227121   0.263102   0.304328   0.349689   0.400498   0.457528  ...   0.758162   0.853498   0.925007   1.002077   1.082913   1.168988   1.266940   1.372563   1.457165   1.548468
d_loss   93.115100  83.120677  73.526299  65.941190  59.110550  53.263872  48.805589  46.254232  45.321438  45.005155  ...  40.133810  37.662550  33.584259  30.416175  26.321939  23.335558  20.838240  19.107938  16.982122  15.234780

[3 rows x 23 columns]
[Training] epoch  25: [7500/14786] | RD Loss: 2.4504 | R:1.1459, D:54.2634 | Aux loss: 250.60 | Batch time: 0.5820 |
[Training] epoch  25: [8000/14786] | RD Loss: 2.4472 | R:1.1450, D:54.1975 | Aux loss: 250.80 | Batch time: 0.5824 |
[Training] epoch  25: [8500/14786] | RD Loss: 2.4470 | R:1.1457, D:54.1693 | Aux loss: 241.59 | Batch time: 0.5824 |
[Training] epoch  25: [9000/14786] | RD Loss: 2.4485 | R:1.1459, D:54.1848 | Aux loss: 260.62 | Batch time: 0.5822 |
[Training] epoch  25: [9500/14786] | RD Loss: 2.4467 | R:1.1458, D:54.1383 | Aux loss: 240.75 | Batch time: 0.5820 |
[Training] epoch  25: [10000/14786] | RD Loss: 2.4490 | R:1.1458, D:54.2384 | Aux loss: 248.63 | Batch time: 0.5819 |
[Training] epoch  25: [10500/14786] | RD Loss: 2.4455 | R:1.1449, D:54.1673 | Aux loss: 252.82 | Batch time: 0.5818 |
[Training] epoch  25: [11000/14786] | RD Loss: 2.4469 | R:1.1455, D:54.1917 | Aux loss: 254.71 | Batch time: 0.5816 |
[Training] epoch  25: [11500/14786] | RD Loss: 2.4432 | R:1.1444, D:54.1946 | Aux loss: 250.67 | Batch time: 0.5817 |
[Training] epoch  25: [12000/14786] | RD Loss: 2.4424 | R:1.1439, D:54.1591 | Aux loss: 259.17 | Batch time: 0.5832 |
[Training] epoch  25: [12500/14786] | RD Loss: 2.4405 | R:1.1441, D:54.1363 | Aux loss: 287.03 | Batch time: 0.5848 |
[Training] epoch  25: [13000/14786] | RD Loss: 2.4399 | R:1.1443, D:54.0635 | Aux loss: 238.08 | Batch time: 0.5857 |
[Training] epoch  25: [13500/14786] | RD Loss: 2.4398 | R:1.1442, D:54.0587 | Aux loss: 243.93 | Batch time: 0.5864 |
[Training] epoch  25: [14000/14786] | RD Loss: 2.4383 | R:1.1440, D:54.0598 | Aux loss: 257.73 | Batch time: 0.5873 |
[Training] epoch  25: [14500/14786] | RD Loss: 2.4391 | R:1.1442, D:54.1139 | Aux loss: 263.75 | Batch time: 0.5872 |
[Evaluating] epoch  25: RD Loss: 2.0801 | best RD loss: 1.3977 | R loss: 0.6813 | D Loss: 53.5440 | Aux loss: 256.8645
[Evaluating] detailed RD performance:
            0.00180     0.00235     0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.322912    0.390232    0.455803   0.546672   0.638847   0.751656   0.864041   1.012256   1.151997   1.323777  ...   1.684655   1.707701   1.915324   2.397075   3.012855   3.776736   4.362128   5.241876   5.538129   6.040412
r_loss     0.116302    0.135863    0.159138   0.185339   0.215589   0.251198   0.292326   0.337925   0.389658   0.447692  ...   0.749129   0.843804   0.916192   0.994546   1.076100   1.164225   1.259668   1.364339   1.455339   1.553675
d_loss   114.783608  108.242218  102.298558  96.355678  92.012484  83.409647  77.258799  70.242797  64.604961  57.073935  ...  23.835062  17.886072  17.315973  20.902073  24.164127  28.031239  27.862234  29.942369  26.383137  24.926313

[3 rows x 23 columns]
[training] In epoch 26, lr: 0.0001, Aux lr: 0.001
[Training] epoch  26: [   0/14786] | RD Loss: 3.6344 | R:1.7223, D:39.5889 | Aux loss: 248.58 | Batch time: 0.7560 |
[Training] epoch  26: [ 500/14786] | RD Loss: 2.4182 | R:1.1373, D:54.4472 | Aux loss: 271.88 | Batch time: 0.5826 |
[Training] epoch  26: [1000/14786] | RD Loss: 2.4157 | R:1.1384, D:54.4121 | Aux loss: 246.90 | Batch time: 0.5829 |
[Training] epoch  26: [1500/14786] | RD Loss: 2.4095 | R:1.1369, D:54.2620 | Aux loss: 243.86 | Batch time: 0.5855 |
[Training] epoch  26: [2000/14786] | RD Loss: 2.4080 | R:1.1387, D:54.2413 | Aux loss: 264.85 | Batch time: 0.5860 |
[Training] epoch  26: [2500/14786] | RD Loss: 2.4097 | R:1.1400, D:54.1686 | Aux loss: 275.49 | Batch time: 0.5873 |
[Training] epoch  26: [3000/14786] | RD Loss: 2.4125 | R:1.1421, D:54.1429 | Aux loss: 262.19 | Batch time: 0.5885 |
[Training] epoch  26: [3500/14786] | RD Loss: 2.4202 | R:1.1430, D:54.4091 | Aux loss: 245.24 | Batch time: 0.5909 |
[Training] epoch  26: [4000/14786] | RD Loss: 2.4188 | R:1.1420, D:54.1049 | Aux loss: 272.23 | Batch time: 0.5915 |
[Training] epoch  26: [4500/14786] | RD Loss: 2.4227 | R:1.1422, D:54.2416 | Aux loss: 279.06 | Batch time: 0.5919 |
[Training] epoch  26: [5000/14786] | RD Loss: 2.4248 | R:1.1430, D:54.1532 | Aux loss: 241.06 | Batch time: 0.5925 |
[Training] epoch  26: [5500/14786] | RD Loss: 2.4277 | R:1.1429, D:54.1825 | Aux loss: 240.22 | Batch time: 0.5924 |
[Training] epoch  26: [6000/14786] | RD Loss: 2.4236 | R:1.1405, D:54.0446 | Aux loss: 248.25 | Batch time: 0.5943 |
[Training] epoch  26: [6500/14786] | RD Loss: 204377593636.9614 | R:1.1414, D:8324952749581.1006 | Aux loss: 277.00 | Batch time: 0.5965 |
[Training] epoch  26: [7000/14786] | RD Loss: 189781279279.4046 | R:1.1416, D:7730398203835.6738 | Aux loss: 236.77 | Batch time: 0.5991 |
[Evaluating] epoch  26: RD Loss: 1.5785 | best RD loss: 1.3977 | R loss: 0.6805 | D Loss: 39.7824 | Aux loss: 239.7165
[Evaluating] detailed RD performance:
            0.00180    0.00235    0.00290    0.00375    0.00460    0.00600    0.00740    0.00960    0.01180    0.01535  ...    0.03925    0.04830    0.05770    0.06710    0.08015    0.09320    0.11135    0.12950    0.15475    0.18000
rd_loss    0.304467   0.356572   0.403392   0.466924   0.524094   0.605272   0.678332   0.777987   0.868213   1.000725  ...   1.817267   2.064032   2.234662   2.382540   2.526381   2.659457   2.830810   2.987017   3.244424   3.551867
r_loss     0.123359   0.143913   0.167621   0.193447   0.223399   0.258907   0.299565   0.343510   0.392907   0.448644  ...   0.739746   0.832948   0.904347   0.982125   1.068367   1.161143   1.259655   1.366060   1.449694   1.542712
d_loss   100.615583  90.492981  81.300398  72.927163  65.368364  57.727359  51.184698  45.257972  40.280138  35.966174  ...  27.452752  25.488282  23.055723  20.870561  18.191069  16.076341  14.110060  12.517046  11.597612  11.161975

[3 rows x 23 columns]
[Training] epoch  26: [7500/14786] | RD Loss: 177130880714.0822 | R:1.1409, D:7215107029073.7021 | Aux loss: 233.57 | Batch time: 0.6016 |
[Training] epoch  26: [8000/14786] | RD Loss: 166061584331.6540 | R:1.1403, D:6764219200738.5869 | Aux loss: 291.21 | Batch time: 0.6039 |
[Training] epoch  26: [8500/14786] | RD Loss: 156294404921.6288 | R:1.1402, D:6366370759338.4521 | Aux loss: 281.87 | Batch time: 0.6059 |
[Training] epoch  26: [9000/14786] | RD Loss: 147612347099.2082 | R:1.1410, D:6012722789152.6445 | Aux loss: 226.96 | Batch time: 0.6069 |
[Training] epoch  26: [9500/14786] | RD Loss: 139844093910.2379 | R:1.1409, D:5696297002967.0430 | Aux loss: 252.16 | Batch time: 0.6063 |
[Training] epoch  26: [10000/14786] | RD Loss: 132852588365.4031 | R:1.1408, D:5411510631458.6455 | Aux loss: 265.36 | Batch time: 0.6058 |
^CTraceback (most recent call last):
  File "train.py", line 273, in <module>
    main(args)
  File "train.py", line 259, in main
    train_loss, best_val_loss = train_epoch(
  File "train.py", line 117, in train_epoch
    out = net(inputs, level, alpha)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/MultiRate_AttModulation/models/net_baseline.py", line 236, in forward
    y = self.g_a(x, level, alpha)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/MultiRate_AttModulation/models/net_modified.py", line 58, in forward
    x = self.ChannelModulator0(x, level, alpha)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/MultiRate_AttModulation/models/modulator.py", line 82, in forward
    x_adapt = self.conv1(x_adapt)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py", line 399, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py", line 395, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt

]0;root@678f520f5371: /home/MultiRate_AttModulationroot@678f520f5371:/home/MultiRate_AttModulation# [K[K]0;root@678f520f5371: /home/MultiRate_AttModulationroot@678f520f5371:/home/MultiRate_AttModulation# 